---
title: "Regression"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(glmnet)
library(leaps)
library(mgcv)
library(ggcorrplot)
library(modelr)
library(GGally)
set.seed(1)

merged_violence_df <- read_csv(
  file = "data/merged_violence.csv",
  col_select = c(2:15))

```

# Introduction
Our motivation for doing regression is to determine the significant predictors 
of the two measures of violence: (1) homicide rate and (2) violence rate. 

Violence is a complex and multifaceted issue, especially when considered in a global context. 
To better understand its various dimensions, we aimed to include a diverse set 
of predictors that reflect different underlying factors driving violence. 
Specifically, economic indicators like GDP, inflation rate, and 
the Human Development Index (HDI) capture aspects of economic inequality, 
which can influence social unrest and violence. Economic crime rate, 
personnel rate, and number of trafficked victims serve as indicators of 
the level of criminal activity in a country, as well as potential efficacy of 
law enforcement. Such indicators could be predictive of violence in the country. 
Lastly, alcohol rate is included to address the possibility that substance 
usage could influence the occurrence of violence.

Given the plausible theoretical linkage between each of our included 
predictor variables and the outcome variables of interest, we seek to develop 
models that can determine which factors are most predictive of homicide and 
violence rate. 

We performed this analysis to uncover the extent to which 
different factors influence homicide rate and violence rate as outcomes.

# Functions for Fitting Data

## Best_subset
```{r }
best_subset = function(predictor, outcome, criterion) {
  
  optimal_subset <- 
    leaps(x = predictor, y = outcome, nbest = 3, 
      method = criterion, names = names(predictor))
  
  if (criterion == "Cp") {
    optimal_criterion <- optimal_subset[[criterion]] %>% min()
    optimal_subset_idx <- optimal_subset[[criterion]] %>% which.min()
    
  } else {
    optimal_criterion <- optimal_subset[[criterion]] %>% max()
    optimal_subset_idx <- optimal_subset[[criterion]] %>% which.max()
  }
  
  
  list(Criterion = optimal_criterion , 
       Variable_Selection = optimal_subset$which[optimal_subset_idx,])
  
}
```
This function serves as a wrapper around the leaps function. It performs 
best subset variable selection and then prints out the optimal model diagnostic 
and the predictor variables included in the regression model.

## fit_glmnet
```{r }
fit_glmnet = function(df, alpha, outcome, lambda) {
  
  outcome_formula <- as.formula(paste(outcome, "~."))
  predictor <- model.matrix(outcome_formula, data = df)[,-1]
  predicted <- df %>% pull(outcome)
  
  model_fit <- 
    glmnet(predictor, predicted, lambda = lambda, alpha = alpha)
  
  model_cv <-
    cv.glmnet(predictor, predicted, lambda = lambda, alpha = alpha)
  
  lambda_opt = model_cv[["lambda.min"]]
  
  model_fit <-
    glmnet(predictor, predicted, lambda = lambda_opt, alpha = alpha)
  
  return(model_fit)
  
}
```
This function serves as a wrapper for the glmnet fitting processing and 
covers both the fitting and cross-validation process. It returns the glmnet 
object produced after fitting with optimal lambda value.

## rmse_glmnet
```{r }
rmse_glmnet = function(model, test, outcome) {
  outcome_formula <- as.formula(paste(outcome, "~."))
  predictor <- model.matrix(outcome_formula, data = test)[,-1]
  predictions <- predict.glmnet(model, model[["lambda"]], newx = predictor,
                               type = "response")
  
  predictions <- as.vector(predictions)
  observed <- test %>% 
    pull(outcome)
  
  return(caret::RMSE(predictions, observed))
  
}
```
This is a wrapper function for the glmnet prediction and evaluation process. 
Given a fitted glmnet model, a test dataset, and an outcome variable of 
interest, this function predicts the avlues in the test dataset and then 
calculates and returns the rmse. 


# Exploratory data analysis 

## Visualizations of distributions
```{r distributions, fig.width = 20, fig.height = 20, warning = FALSE, message = FALSE, echo = FALSE}
plot_distributions = function(column, name) {
  if(is.numeric(column) & name != "year") {
    ggplot(merged_violence_df, aes(x = column)) +
      geom_density() +
      labs(title = paste("Distribution of", name))
  }
}

merged_violence_df |>
  select(homicide_rate:alcohol_consumption_rate) |>
  ggpairs()
```

All variables are skewed right, except for human development index which is 
bimodal and slightly left-skewed. Hence, we decided to apply ln transformations 
and Box-Cox transformations to these variables in order to account for concerns 
regarding the constancy of variance in linear models. Transformations are 
intended to approximately normalize the data.

## Transformations
Transformation step involves writing a function for natural log transformation 
and a function for Box-Cox transformation. The functions are mapped into the 
nested list col which includes all continuous variables in 
the `merged_violence_df` dataset. 

## Natural Log Transform.
```{r ln-transform, fig.width = 20, fig.height = 20, warning = FALSE}
ln_transform = function(value) {
  return(log(abs(value)))
}

ln_df = 
  merged_violence_df |>
  mutate(across(c(homicide_rate:alcohol_consumption_rate), 
                ln_transform))

ln_df |>
  select(homicide_rate:alcohol_consumption_rate) |>
  ggpairs()

```

## Box-Cox Transform. 
```{r boxcox, fig.width = 20, fig.height = 20, warning = FALSE}
boxcox_transform = function(value) {
  if (all(is.na(value))) {
    return(value) 
  }

  min_value = min(value, na.rm = TRUE) 
  if (min_value <= 0) {
    value = value + abs(min_value) + 0.00001  
  }

  if (length(unique(value)) == 1) {
    return(value) 
  }

  boxcox_result = MASS::boxcox(value ~ 1, plotit = FALSE)
  lambda = boxcox_result$x[which.max(boxcox_result$y)]  

  if(lambda != 0) {
    transformed_value = (value^lambda - 1) / lambda
  } else {
    transformed_value = log(value)
  }
  return(transformed_value)
}

boxcox_df = merged_violence_df |> 
  mutate(across(c(homicide_rate:alcohol_consumption_rate), 
                ~ boxcox_transform(.)))

boxcox_df |>
  select(homicide_rate:alcohol_consumption_rate) |>
  ggpairs()

```

While the initial EDA suggested that logarithmic and Box-Cox transformations 
might be useful for improving normality, the results indicate that these 
transformations are not entirely necessary. The primary motivation for using 
these transformations is often to address skewness. 
However, after reviewing the data and the results of the transformations, 
we found that the original data already provides a reasonable representation of
the underlying distribution.


While the natural logarithm helped with normality, the Box-Cox transformation 
did not substantially improve the data's distribution. Box-Cox transformation
requires shifting data to be strictly positive, which could introduce 
unnecessary complexity.


Box-Cox and ln transformations were performed with intentions of improving 
the approximate normality of each of the predictor distributions. However, 
neither transformation substantially approved the shapes of the distributions.
Since no major improvements in normality were observed with the transformations, 
we ultimately decided to use the original data. Concerns regarding the 
normality of the distributions are alleviated by the Central Limit Theorem. 
With a sufficiently large amount of data points, we can assume that the sampling 
distribution is approximately normal. Using the convention that n > 30 allows for 
the assumption of approximate normality, our 310 observation dataset suffices
to be supported by CLT.

## Multicolliniearity diagnostics
We used `cor()` to find the correlation between the eight predictors of homicide rate and violence. 
```{r echo=FALSE}
cor_matrix = 
  cor(merged_violence_df[, c("gdp", "inflation_rate", "unemployment_rate", "hdi","crime_rate", "personnel_rate", "trafficked_victims", "alcohol_consumption_rate")], use = "pairwise.complete.obs")


ggcorrplot(cor_matrix, 
           method = "circle",  
           type = "lower",  
           lab = TRUE,        
           lab_size = 3,      
           colors = c("blue", "white", "red"), # Color scale (blue = negative, red = positive)
           title = "Correlation Heatmap"
)
```

There is moderate correlation between `hdi` and `alcohol_consumption_rate`, 
corr = 0.56, and moderately high correlation between `trafficked_victims` and 
`gdp`, corr = 0.77. 

# Data Pre-processing

In this step, we manipulate the merged_violence_df in preparation for later 
statistical modelling. 

## Define Lambda Range
```{r}
lambda = 10^(seq(-2, 2.75, 0.1))
```

## Approach to pre-processing data 

Prior to performing any model fitting, we split our intial dataset into 
separate homicide and violence datasets, each of which includes the outcome 
variable and all numeric predictor variables. Specific predictor and outcome 
dataframes, and an outcome matrix, are created for later use in linear models, 
as well as lasso.

## Pre-processing for homicide.
```{r}
homicide_df =
  merged_violence_df |> 
  ungroup() |> 
  select(
    homicide_rate, everything(), -violence_rate, -year, -country, -region, 
    -iso3_code) |> 
  drop_na()

homicide_matrix <- model.matrix(homicide_rate ~., data = homicide_df)[,-1]

homicide_predictors <- homicide_df %>% 
  select(-homicide_rate)

homicide_outcome <- homicide_df %>% 
  pull(homicide_rate)
```

## Pre-processing for violence.
```{r }
violence_df =
  merged_violence_df |> 
  ungroup() |> 
  select(
    violence_rate, everything(), -year, -homicide_rate, -country, -region, 
    -iso3_code) |>
  drop_na()

violence_matrix <- model.matrix(violence_rate ~., data = violence_df)[,-1]
  
violence_predictors <- violence_df %>% 
  select(-violence_rate)

violence_outcome <- violence_df %>% 
  pull(violence_rate)

```

# Baseline MLR with additive effects

First, we will fit a baseline MLR with additive effects from all possible 
predictor variables to use as a point of reference for our modeling process. 
This step provides basic understanding of the extent to which different 
covariates contribute to the outcomes of interest. 

## Predict Homicide Rates
```{r echo=FALSE}
baseline_MLR_hom <- lm(homicide_rate ~ gdp + inflation_rate + unemployment_rate +
                   hdi + crime_rate + personnel_rate + trafficked_victims + 
                    alcohol_consumption_rate , data = merged_violence_df)
baseline_MLR_hom %>% 
  broom::tidy() %>% 
  knitr::kable()
```
Results from baseline MLR show that that inflation rate, unemployment rate and 
hdi are significant predictors of homicide rate at 
alpha = 0.05. 

## Check Baseline Homicide Prediction Model for Colinearity
```{r echo=FALSE}
vif_baseline_hom_MLR <- car::vif(baseline_MLR_hom)
vif_baseline_hom_MLR %>% 
  tibble(
    variable = names(vif_baseline_hom_MLR),
    VIF = vif_baseline_hom_MLR
  ) %>% 
  select(variable, VIF) %>% 
  knitr::kable()

```

Results from VIF show that there is no multicollinearity issue (VIF < 5).

## Predict Violence Rates
```{r echo=FALSE}
baseline_MLR_viol <- lm(violence_rate ~ gdp + inflation_rate + unemployment_rate +
                   hdi + crime_rate + personnel_rate + trafficked_victims + 
                    alcohol_consumption_rate , data = merged_violence_df)
baseline_MLR_viol %>% 
  broom::tidy() %>% 
  knitr::kable()
```
Results from MLR show that gdp, inflation rate, crime rate, personnel rate, 
number of trafficked victims per 100,000 and alcohol consumption rates are 
significant predictors of violence rate at significance level alpha = 0.01. 

## Calculate VIF for baseline violence MLR
```{r echo=FALSE}
vif_baseline_viol_MLR <- car::vif(baseline_MLR_viol)
vif_baseline_viol_MLR %>% 
  tibble(
    variable = names(vif_baseline_viol_MLR),
    VIF = vif_baseline_viol_MLR
  ) %>% 
  select(variable, VIF) %>% 
  knitr::kable()
```
Results from VIF show that there is no multicollinearity issue (VIF < 5).

# Lasso 
We used Lasso to as a feature selection tool to find the most important 
variables in predicting homicide rate and violence rate. 

## Predictors for violence rate:
```{r echo=FALSE}
lasso_violence_fit = 
  glmnet(violence_matrix, violence_outcome, lambda = lambda)

lasso_violence_cv = 
  cv.glmnet(violence_matrix, violence_outcome, lambda = lambda)

lambda_violence_opt = 
  lasso_violence_cv[["lambda.min"]]

lasso_violence_fit = 
  glmnet(violence_matrix, violence_outcome, lambda = lambda_violence_opt)

lasso_violence_fit |> 
  broom::tidy() |> 
  knitr::kable()

lasso_predict_violence <- 
  predict.glmnet(lasso_violence_fit, lambda_violence_opt, 
                 newx = violence_matrix, type = "response")
  
lasso_predict_violence <- as.vector(lasso_predict_violence)

lasso_violence_resid <- tibble(
  residuals = violence_outcome - lasso_predict_violence
)

```

The optimal lambda for violence rate is `r lambda_violence_opt`` Based on 
lasso estimates, the coefficient for `gdp` was shrunk to 0.


## Predictors for homicide rate.

```{r echo=FALSE}
lasso_homicide_fit = 
  glmnet(homicide_matrix, homicide_outcome, lambda = lambda)

lasso_homicide_cv = 
  cv.glmnet(homicide_matrix, homicide_outcome, lambda = lambda)

lambda_homicide_opt = 
  lasso_homicide_cv[["lambda.min"]]

lasso_homicide_fit = 
  glmnet(homicide_matrix, homicide_outcome, lambda = lambda_homicide_opt)

lasso_homicide_fit |> 
  broom::tidy() |> 
  knitr::kable()

lasso_predict_homicide <- 
  predict.glmnet(lasso_homicide_fit, lambda_homicide_opt, 
                 newx = homicide_matrix, type = "response")
  
lasso_predict_homicide <- as.vector(lasso_predict_homicide)

lasso_homicide_resid <- tibble(
  residuals = homicide_outcome - lasso_predict_homicide
)

```

The optimal lambda for homicide rate is `r lambda_homicide_opt`. Similar 
to the results from violence, the coefficient for `gdp` was shrunk to 0. 

# Criterion-based procedure

## Best Subset Regression for Violence Rate

### Predict violence rate using r-squared as criterion
```{r echo=FALSE}
best_subset(predictor = violence_predictors, outcome = violence_outcome, 
            criterion = "adjr2")
```

### Predict violence_rate using Cp as criterion
```{r echo=FALSE}
best_subset(predictor = violence_predictors, outcome = violence_outcome, 
            criterion = "Cp")
```

### Check for co-linearity in best subset violence model 
```{r echo=FALSE}
subset_violence_lm <- lm(violence_rate ~ gdp + inflation_rate + 
                           crime_rate + personnel_rate + trafficked_victims +
                           alcohol_consumption_rate, data = violence_df)
vif_subset_violence_model <- car::vif(subset_violence_lm) 

vif_subset_violence_model %>% 
  tibble(
    variable = names(vif_subset_violence_model),
    VIF = vif_subset_violence_model
  ) %>% 
  select(variable, VIF) %>% 
  knitr::kable()
```
All variables have VIF value below 5, suggesting that there is no 
multi-colinearity concerns.

### Interpretation
The results from criterion-based procedures suggest that 
significant predictors for violence_rate are gdp, inflation_rate, crime_rate, personnel_rate, trafficked_victims and alcohol_consumption rate, with a total of 6 predictors. 
This yields the most appropriate Cp value (5.363), which is approximately 
close to the number of predictors, and highest adjusted R-squared (0.510).

## Best subset regression for Homicide Rate

## Best subset regression for homicide rate

### Predict homicide rate using R-squared as criterion
```{r echo=FALSE}
best_subset(predictor = homicide_predictors, outcome = homicide_outcome, 
            criterion = "adjr2")
```

### Predict Homicide Rates using Cp as criterion
```{r echo=FALSE}
best_subset(predictor = homicide_predictors, outcome = homicide_outcome, 
            criterion = "Cp")
```

### Check for co-linearity in best subset homicide model
```{r echo=FALSE}
subset_homicide_lm <- lm(homicide_rate ~ inflation_rate + unemployment_rate +
                           hdi + crime_rate, data = homicide_df)

vif_subset_homicide_model <- car::vif(subset_homicide_lm)
vif_subset_homicide_model %>% 
  tibble(
    variable = names(vif_subset_homicide_model),
    VIF = vif_subset_homicide_model
  ) %>% 
  select(variable, VIF) %>% 
  knitr::kable()
```
All variables have VIF value below 5, suggesting that there is no 
multi-colinearity concerns.


### Interpretation
For homicide_rate, the results are not as straightforward. 
The best model based on Cp and R-squared seems to be the model with 
4 predictors because it has the lowest Cp (6.36), and a relatively high 
adjusted R-squared (0.331), indicating a good balance between fit and 
complexity. As more predictors are added, R-squared adjusts slightly but Cp 
levels off at around 6 predictors. The model with 4 predictors, which includes 
inflation_rate, unemployment_rate, hdi, personnel_rate, seems to be a 
better trade-off.


# Model Comparison 

## Create Training and Testing Datasets
```{r}
cv_df_violence <- 
  modelr::crossv_mc(violence_df, 100)

cv_df_violence <- cv_df_violence %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

cv_df_homicide <-
  modelr::crossv_mc(homicide_df, 100)

cv_df_homicide <- cv_df_homicide %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

```

## Fit Models

### Violence
```{r}
cv_df_violence <-
  cv_df_violence %>% 
  mutate(
    MLR_all_var = map(train, \(df) lm(violence_rate ~ ., data = df)),
    MLR_best_subset = map(train, \(df) lm(violence_rate ~ gdp + inflation_rate + 
                           crime_rate + personnel_rate + trafficked_victims +
                           alcohol_consumption_rate, data = df)),
    lasso = map(train, \(df) fit_glmnet(df, outcome = "violence_rate", 
                                        alpha = 1,lambda = lambda))
  ) %>% 
  mutate(
    rmse_all_var = map2_dbl(
      MLR_all_var, test, \(mod, test) rmse(model = mod, data = test)),
    rmse_best_subset = map2_dbl(
      MLR_best_subset, test, \(mod, test) rmse(model = mod, data = test)),
    rmse_lasso = map2_dbl(
      lasso, test, \(mod, test) rmse_glmnet(mod, test, "violence_rate"))
  ) %>% 
  select(starts_with("rmse_"))

```

### Homicide
```{r}
cv_df_homicide <-
  cv_df_homicide %>% 
  mutate(
    MLR_all_var = map(train, \(df) lm(homicide_rate ~ ., data = df)),
    MLR_best_subset = map(train, \(df) lm(homicide_rate ~ inflation_rate 
                          + unemployment_rate + hdi + crime_rate, data = df)),
    lasso = map(train, \(df) fit_glmnet(df, outcome = "homicide_rate", 
                                        alpha = 1,lambda = lambda))
  ) %>% 
  mutate(
    rmse_all_var = map2_dbl(
      MLR_all_var, test, \(mod, test) rmse(model = mod, data = test)),
    rmse_best_subset = map2_dbl(
      MLR_best_subset, test, \(mod, test) rmse(model = mod, data = test)),
    rmse_lasso = map2_dbl(
      lasso, test, \(mod, test) rmse_glmnet(mod, test, "homicide_rate"))
  ) %>% 
  select(starts_with("rmse_"))
```


## Compare RMSE

### Violence
```{r echo=FALSE}
cv_df_violence %>%
  pivot_longer(
    everything(),
    names_to = "Model", 
    values_to = "RMSE",
    names_prefix = "rmse_"
  ) %>% 
  mutate(
    Model = fct_inorder(Model)
  ) %>% 
  ggplot(aes(x = Model, y = RMSE)) +
  geom_violin() +
  ggtitle("RMSE by Model for Predicting Violence Rate") +
  theme(plot.title = element_text(hjust = 0.5))

```
Examining the distribution of RMSEs for each of the 3 models for 
predicting violence rate, we see that they are relatively similar. It appears 
that the MLR including all predictor variables has greater spread than the 
other two distributions, as it has a higher upper bound for in the violin plot.
The similarity in performance between all 3 models likely relates to the few
number of predictor variables that exist in our studied dataset. In general,
each of the 3 models will not differ by many predictor variables, and therefore
perform similaly. Comparing the 3 models, the best subset regression model
may be considered optimal as it reduces model complexity, without the 
further caveat of difficulty interpreting beta coefficients, as is the case 
with lasso models.


### Homicide
```{r echo=FALSE}
cv_df_homicide %>%
  pivot_longer(
    everything(),
    names_to = "Model", 
    values_to = "RMSE",
    names_prefix = "rmse_"
  )%>% 
  mutate(
    Model = fct_inorder(Model)
  ) %>% 
  ggplot(aes(x = Model, y = RMSE)) +
  geom_violin() +
  ggtitle("RMSE by Model for Predicting Homicide Rate") +
  theme(plot.title = element_text(hjust = 0.5))
```

Examining the three models we used to predict homicide rate, we see that 
the distribution of RMSE across 100-fold cross-validation are nearly identical. 
This is likely in regard to the few predictor variables included in our dataset.
In particular, with such few predictor variables, lasso is likely not most 
well-suited for this task, as there is not a serious need to reduce model 
complexity. Consequently, the number of variables included in each of the 3 
models is quite similar, and the difference of 1 or 2 included predictors 
between each of the models does not produce a substantial change in model 
effectiveness. Comparing the 3 models, the best subset regression model 
may be considered the best as it reduces model complexity but maintains 
ease of interpretability of beta coefficients.


# Model Diagnostics

## QQ Plot of All Variable Linear Model

### Violence Rates

```{r echo=FALSE}
resid_df <- tibble(residuals = resid(baseline_MLR_viol)) 

resid_df %>% 
  ggplot(aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line(col = "red")

```


### Homicide Rates
```{r echo=FALSE}
resid_df <- tibble(residuals = resid(baseline_MLR_hom)) 

resid_df %>% 
  ggplot(aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line(col = "red")

```

## QQ Plot of Best Subset Linear Model 

### Violence Rate
```{r echo=FALSE}
resid_df <- tibble(residuals = resid(subset_violence_lm)) 

resid_df %>% 
  ggplot(aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line(col = "red")

```

### Homicide Rate
```{r echp=FALSE}
resid_df <- tibble(residuals = resid(subset_homicide_lm)) 

resid_df %>% 
  ggplot(aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line(col = "red")

```

## QQ Plot of Lasso Model


### Violence Rate
```{r echo=FALSE}
lasso_violence_resid %>% 
  ggplot(aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line(col = "red")

```

### Homicide Rate
```{r echo=FALSE}
lasso_homicide_resid %>% 
  ggplot(aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line(col = "red")

```


# Conclusion 



