---
title: "Regression"
author: "My An Huynh"
date: "2024-11-15"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(MASS)
library(glmnet)
library(leaps)
library(mgcv)
set.seed(1)
```

#Introduction
Our motivation for doing regression is to determine the significant contributors to two outcomes of violence - homicide rate and violence rate. 

Violence is a complex and multifaceted issue, especially when considered in a global context. To better understand its various dimensions, we aimed to include a diverse set of predictors that reflect different underlying factors driving violence. Specifically, economic indicators like GDP, inflation rate, and the Human Development Index (HDI) capture aspects of economic inequality, which can influence social unrest and violence. Crime rate and personnel rate serve as proxies for the level of criminal activity and law enforcement presence, respectively, which can contribute to both non-violent and violent crime. Additionally, trafficked victims and alcohol consumption rate are included to explore the role of cultural norms and behaviors, such as organized crime, substance abuse, and social vulnerability, which can be key drivers of violence.

We hypothesize that homicide rate may be influenced by more "direct" indicators including crime rate, HDI and trafficked_victims. We hypothesize that violence rate may be influenced by all factors we included in this dataset, because violence can exist in multiple forms. 

We perform this analysis to uncover the extent to which different factors influence homicide rate and violence rate as outcomes.

# Helpful Functions
```{r}
best_subset = function(predictor, outcome, criterion) {
  
  optimal_subset <- 
    leaps(x = predictor, y = outcome, nbest = 3, 
      method = criterion, names = names(predictor))
  
  if (criterion == "Cp") {
    optimal_rsq <- optimal_subset[[criterion]] %>% min()
    optimal_subset_idx <- optimal_subset[[criterion]] %>% which.min()
  } else {
    optimal_rsq <- optimal_subset[[criterion]] %>% max()
    optimal_subset_idx <- optimal_subset[[criterion]] %>% which.max()
  }
  
  print(optimal_rsq) 
  print(optimal_subset$which[optimal_subset_idx,])

}

fit_glmnet = function(df, alpha, outcome, lambda) {
  
  outcome_formula <- as.formula(paste(outcome, "~."))
  predictor <- model.matrix(outcome_formula, data = df)[,-1]
  predicted <- df %>% pull(outcome)
  
  model_fit <- 
    glmnet(predictor, predicted, lambda = lambda, alpha = alpha)
  
  model_cv <-
    cv.glmnet(predictor, predicted, lambda = lambda, alpha = alpha)
  
  lambda_opt = model_cv[["lambda.min"]]
  
  model_fit <-
    glmnet(predictor, predicted, lambda = lambda_opt, alpha = alpha)
  
}

rmse_glmnet = function(model, test, outcome) {
  outcome_formula <- as.formula(paste(outcome, "~."))
  predictor <- model.matrix(outcome_formula, data = test)[,-1]
  predictions <- predict.glmnet(model, model[["lambda"]], newx = predictor,
                               type = "response")
  
  predictions <- as.vector(predictions)
  observed <- test %>% 
    pull(outcome)
  
  return(caret::RMSE(predictions, observed))
  
}
```

This function serves as a wrapper around the leaps function. It performs 
best subset variable selection and then prints out the optimal model diagnostic 
and the predictor variables included in the regression model. 

# Exploratory data analysis 

## Visualizations of distributions
```{r train/test + distribution}
##train_df = sample_frac(merged_violence_df, size = 0.8)
##test_df = anti_join(merged_violence_df, train_df)

plot_distributions = function(column, name) {
  if(is.numeric(column) & name != "year") {
    ggplot(merged_violence_df, aes(x = column)) +
    geom_density() +
    labs(title = paste("Distribution of", name))
  }
}

list = colnames(merged_violence_df)
map(list, \(x) plot_distributions(pull(merged_violence_df, x), x))
```

All variables are skewed right, except for human development index which is bimodal and slightly left-skewed. We will apply ln transformations and Box-Cox transformations to these variables. Even though it is not necessary to normalize the distribution of predictors, this step will stabilize variance and reduce heteroscedasticity. We believe it will be helpful for further steps with model.

## Transformations
Transformation step involves writing a function for natural log transformation and a function for Box-Cox transformation. The functions will be mapped into the 
nested list col which includes all continuous variables in the `merged_violence_df` dataset. 

## Natural Log Transform.
```{r ln_transform}
ln_transform = function(value) {
  return(log(abs(value)))
}

ln_df = 
  merged_violence_df |>
  mutate(across(c(homicide_rate:alcohol_consumption_rate), 
                ln_transform))

map(list, \(x) plot_distributions(pull(ln_df, x), x))
```

## Box-Cox Transform. 
```{r boxcox}
boxcox_transform = function(value) {
  if (all(is.na(value))) {
    return(value) 
  }

  min_value = min(value, na.rm = TRUE) 
  if (min_value <= 0) {
    value = value + abs(min_value) + 0.00001  
  }

  if (length(unique(value)) == 1) {
    return(value) 
  }

  boxcox_result = boxcox(value ~ 1, plotit = FALSE)
  lambda = boxcox_result$x[which.max(boxcox_result$y)]  


  transformed_value = (value^lambda - 1) / lambda
  return(transformed_value)
}


boxcox_df = merged_violence_df |> 
  mutate(across(c(homicide_rate:alcohol_consumption_rate), 
                ~ boxcox_transform(.)))

map(list, \(x) plot_distributions(pull(boxcox_df, x), x))

```

While initial EDA suggested that logarithmic and Box-Cox transformations might be useful for improving normality, the results indicate that these transformations are not entirely necessary. The primary motivation for using these transformations is often to address skewness. However, after reviewing the data and the results of the transformations, we found that the original data already provides a reasonable representation of the underlying distribution.

While the natural logarithm helped with normality, the Box-Cox transformation did not substantially improve the data's distribution. Box-Cox transformation requires shifting data to be strictly positive, which could introduce unnecessary complexity.

Since no major improvements in normality were observed with the transformations, we ultimately decided that it would be more appropriate to use the original data. This approach avoids unnecessary manipulation and preserves the integrity of the underlying data.

## Multicolliniearity diagnostics
We used `cor()` to find the correlation between the eight predictors of homicide rate and violence. 
```{r}
cor_matrix = 
  cor(merged_violence_df[, c("gdp", "inflation_rate", "unemployment_rate", "hdi","crime_rate", "personnel_rate", "trafficked_victims", "alcohol_consumption_rate")], use = "pairwise.complete.obs")


ggcorrplot(cor_matrix, 
           method = "circle",  
           type = "lower",  
           lab = TRUE,        
           lab_size = 3,      
           colors = c("blue", "white", "red"), # Color scale (blue = negative, red = positive)
           title = "Correlation Heatmap"
)
```
There is moderate correlation between `hdi` and `alcohol_consumption_rate`, corr = 0.5, and moderately high correlation between `trafficked_victims` and `gdp`, corr = 0.7. 

# Statistical Learning Approaches

## Data Pre-processing

### Define Lambda Range
```{r}
lambda = 10^(seq(-2, 2.75, 0.1))
```

### Pre-processing for homicide.
```{r}
homicide_df =
  merged_violence_df |> 
  dplyr::ungroup() |> 
  dplyr::select(
    homicide_rate, everything(), -violence_rate, -year, -country, -region, 
    -iso3_code) |> 
  drop_na()

homicide_matrix <- model.matrix(homicide_rate ~., data = homicide_df)[,-1]

homicide_predictors <- homicide_df %>% 
  dplyr::select(-homicide_rate)

homicide_outcome <- homicide_df %>% 
  pull(homicide_rate)
```

###Pre-processing for violence.
```{r}
violence_df =
  merged_violence_df |> 
  dplyr::ungroup() |> 
  dplyr::select(
    violence_rate, everything(), -year, -homicide_rate, -country, -region, 
    -iso3_code) |>
  drop_na()

violence_matrix <- model.matrix(violence_rate ~., data = violence_df)[,-1]
  
violence_predictors <- violence_df %>% 
  dplyr::select(-violence_rate)

violence_outcome <- violence_df %>% 
  pull(violence_rate)

```


# Baseline MLR with additive effects
First, we will fit a baseline MLR with additive effects from all possible predictor variables to use as a point of reference for our modeling process. This step provides basic understanding of the extent to which different covariates contribute to the outcomes of interest. 

## Predict Homicide Rates
```{r}
baseline_MLR_hom <- lm(homicide_rate ~ gdp + inflation_rate + unemployment_rate +
                   hdi + crime_rate + personnel_rate + trafficked_victims + 
                    alcohol_consumption_rate , data = merged_violence_df)
summary(baseline_MLR_hom)
```
Results from baseline MLR show that that inflation rate, unemployment rate and hdi are significant predictors of homicide rate at significance level alpha = 0.05. 

## Check Baseline Homicide Prediction Model for Colinearity
```{r}
vif_baseline_hom_MLR <- car::vif(baseline_MLR_hom)
vif_baseline_hom_MLR %>% 
  tibble(
    variable = names(vif_baseline_hom_MLR),
    VIF = vif_baseline_hom_MLR
  ) %>% 
  dplyr::select(variable, VIF) %>% 
  knitr::kable()

```

Results from VIF show that there is no multicollinearity issue (VIF < 5).

## Predict Violence Rates
```{r}
baseline_MLR_viol <- lm(violence_rate ~ gdp + inflation_rate + unemployment_rate +
                   hdi + crime_rate + personnel_rate + trafficked_victims + 
                    alcohol_consumption_rate , data = merged_violence_df)
summary(baseline_MLR_viol)

```
Results from MLR show that gdp, inflation rate, crime rate, personnel rate, number of trafficked victims per 100,000 and alcohol consumption rates are significant predictors of violence rate at significance level alpha = 0.01. 

## Calculate VIF for baseline violence MLR
```{r}
vif_baseline_viol_MLR <- car::vif(baseline_MLR_viol)
vif_baseline_viol_MLR %>% 
  tibble(
    variable = names(vif_baseline_viol_MLR),
    VIF = vif_baseline_viol_MLR
  ) %>% 
  dplyr::select(variable, VIF) %>% 
  knitr::kable()
```
Results from VIF show that there is no multicollinearity issue (VIF < 5).

# Lasso 
We used Lasso to as a feature selection tool to find the most important variables in predicting homicide rate and violence rate. 

## Predictors for violence rate:
To prepare data for Lasso (fitting violence rate), we removed variables that are not predictors including iso3_code, country, region, year and homicide rate. We then dropped NA values before performing Lasso. 

```{r}
lasso_violence_fit = 
  glmnet(violence_matrix, violence_outcome, lambda = lambda)

lasso_violence_cv = 
  cv.glmnet(violence_matrix, violence_outcome, lambda = lambda)

lambda_violence_opt = 
  lasso_violence_cv[["lambda.min"]]

lasso_violence_fit = 
  glmnet(violence_matrix, violence_outcome, lambda = lambda_violence_opt)

lasso_violence_fit |> 
  broom::tidy() |> 
  knitr::kable()
```

The optimal lambda for violence rate is 0.794. Based on lasso estimates, the coefficient for `gdp` was shrunk to 0. 


## Predictors for homicide rate.

```{r}
lasso_homicide_fit = 
  glmnet(homicide_matrix, homicide_outcome, lambda = lambda)

lasso_homicide_cv = 
  cv.glmnet(homicide_matrix, homicide_outcome, lambda = lambda)

lambda_homicide_opt = 
  lasso_homicide_cv[["lambda.min"]]

lasso_homicide_fit = 
  glmnet(homicide_matrix, homicide_outcome, lambda = lambda_homicide_opt)

lasso_homicide_fit |> 
  broom::tidy() |> 
  knitr::kable()
```

The optimal lambda for homicide rate is 0.050. Similar to the results from violence, the coefficient for `gdp` was shrunk to 0. 

# Criterion-based procedure

## Best Subset Regression for Violence Rate

### Predict violence rate using r-squared as criterion
```{r}
best_subset(predictor = violence_predictors, outcome = violence_outcome, 
            criterion = "adjr2")
```

### Predict violence_rate using Cp as criterion
```{r}
best_subset(predictor = violence_predictors, outcome = violence_outcome, 
            criterion = "Cp")
```

### Check for co-linearity in best subset violence model 
```{r}
subset_violence_lm <- lm(violence_rate ~ gdp + inflation_rate + 
                           crime_rate + personnel_rate + trafficked_victims +
                           alcohol_consumption_rate, data = violence_df)
vif_subset_violence_model <- car::vif(subset_violence_lm) 

vif_subset_violence_model %>% 
  tibble(
    variable = names(vif_subset_violence_model),
    VIF = vif_subset_violence_model
  ) %>% 
  dplyr::select(variable, VIF) %>% 
  knitr::kable()
```

## Best subset regression for Homicide Rate

### Predict homicide rate using R-squared as criterion
```{r}
best_subset(predictor = homicide_predictors, outcome = homicide_outcome, 
            criterion = "adjr2")
```

### Predict Homicide Rates using Cp as criterion
```{r}
best_subset(predictor = homicide_predictors, outcome = homicide_outcome, 
            criterion = "Cp")
```


### Check for co-linearity in best subset homicide model
```{r}
subset_homicide_lm <- lm(homicide_rate ~ inflation_rate + unemployment_rate +
                           hdi + crime_rate, data = homicide_df)

vif_subset_homicide_model <- car::vif(subset_homicide_lm)
vif_subset_homicide_model %>% 
  tibble(
    variable = names(vif_subset_homicide_model),
    VIF = vif_subset_homicide_model
  ) %>% 
  dplyr::select(variable, VIF) %>% 
  knitr::kable()
```

The results from criterion-based procedures suggest that significant predictors for violence_rate are gdp, inflation_rate, crime_rate, personnel_rate, trafficked_victims and alcohol_consumption rate, with a total of 6 predictors. This yields the most appropriate Cp value (5.363), which is approximately close to the number of predictors, and highest adjusted R-squared (0.510). 

For homicide_rate, the results are not as straightforward. The best model based on Cp and R-squared seems to be the model with 4 predictors because it has the lowest Cp (6.36), and a decently high adjusted R-squared (0.331), indicating a good balance between fit and complexity. As more predictors are added, R-squared adjusts slightly but Cp levels off at around 6 predictors. The model with 4 predictors, which includes inflation_rate, unemployment_rate, hdi, personnel_rate, seems to be a better trade-off.


# Model Comparison 

## Create Training and Testing Datasets
```{r}
cv_df_violence <- 
  modelr::crossv_mc(violence_df, 100)

cv_df_violence <- cv_df_violence %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

cv_df_homicide <-
  modelr::crossv_mc(homicide_df, 100)

cv_df_homicide <- cv_df_homicide %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

```

## Fit Models

### Violence
```{r}
cv_df_violence <-
  cv_df_violence %>% 
  mutate(
    MLR_all_var = map(train, \(df) lm(violence_rate ~ ., data = df)),
    MLR_best_subset = map(train, \(df) lm(violence_rate ~ gdp + inflation_rate + 
                           crime_rate + personnel_rate + trafficked_victims +
                           alcohol_consumption_rate, data = df)),
    lasso = map(train, \(df) fit_glmnet(df, outcome = "violence_rate", 
                                        alpha = 1,lambda = lambda))
  ) %>% 
  mutate(
    rmse_all_var = map2_dbl(
      MLR_all_var, test, \(mod, test) rmse(model = mod, data = test)),
    rmse_best_subset = map2_dbl(
      MLR_best_subset, test, \(mod, test) rmse(model = mod, data = test)),
    rmse_lasso = map2_dbl(
      lasso, test, \(mod, test) rmse_glmnet(mod, test, "violence_rate"))
  ) %>% 
  dplyr::select(starts_with("rmse_"))

```

### Homicide
```{r}
cv_df_homicide <-
  cv_df_homicide %>% 
  mutate(
    MLR_all_var = map(train, \(df) lm(homicide_rate ~ ., data = df)),
    MLR_best_subset = map(train, \(df) lm(homicide_rate ~ inflation_rate 
                          + unemployment_rate + hdi + crime_rate, data = df)),
    lasso = map(train, \(df) fit_glmnet(df, outcome = "homicide_rate", 
                                        alpha = 1,lambda = lambda))
  ) %>% 
  mutate(
    rmse_all_var = map2_dbl(
      MLR_all_var, test, \(mod, test) rmse(model = mod, data = test)),
    rmse_best_subset = map2_dbl(
      MLR_best_subset, test, \(mod, test) rmse(model = mod, data = test)),
    rmse_lasso = map2_dbl(
      lasso, test, \(mod, test) rmse_glmnet(mod, test, "homicide_rate"))
  ) %>% 
  dplyr::select(starts_with("rmse_"))
```


## Compare RMSE

### Violence
```{r}
cv_df_violence %>%
  pivot_longer(
    everything(),
    names_to = "Model", 
    values_to = "RMSE",
    names_prefix = "RMSE_"
  ) %>% 
  mutate(
    Model = fct_inorder(Model)
  ) %>% 
  ggplot(aes(x = Model, y = RMSE)) +
  geom_violin()

```

### Homicide
```{r}
cv_df_homicide %>%
  pivot_longer(
    everything(),
    names_to = "Model", 
    values_to = "RMSE",
    names_prefix = "RMSE_"
  )%>% 
  mutate(
    Model = fct_inorder(Model)
  ) %>% 
  ggplot(aes(x = Model, y = RMSE)) +
  geom_violin()
```

