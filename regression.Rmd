---
title: "**Regression**"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
library(tidyverse)
library(glmnet)
library(leaps)
library(mgcv)
library(ggcorrplot)
library(modelr)
library(GGally)

theme_set(theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5)))

set.seed(1)

merged_violence_df <- read_csv(
  file = "data/merged_violence.csv",
  col_select = c(2:15))
```

# *Background*
Our motivation for doing regression is to determine the significant predictors 
of the two measures of violence: (1) homicide victim rate and (2) violent offences rate. 

Global violence is a complex issue, depending on many factors, from political,
cultural, to economic. We aimed to choose predictors that reflect inequality which are
often related to social unrest and violence. Much numerical data exists for the economic indicators of a country
such as GDP, inflation and unemployment rate, and we also wanted to include HDI since that in itself
is a well-recognized global measurement. Intuitively, criminal activity is also related to violence,
so indicators for economic crime rate and criminal justice personnel were considered. Lastly, alcohol rate was included
because substance usage is often perceived to be correlated with violence. Subsequently, 
we aim to develop models that show which factors are the best predictors of homicide and violence rates. 

Given the plausible theoretical linkage between each of our included 
predictor variables and the outcome variables of interest, we seek to develop 
models that can determine which factors are most predictive of homicide and 
violence rate. 

We performed this analysis to uncover the extent to which 
different factors influence homicide rate and violence rate as outcomes.

# *Functions for Fitting Data*

## best_subset
```{r }
best_subset = function(predictor, outcome, criterion) {
  
  optimal_subset <- 
    leaps(x = predictor, y = outcome, nbest = 3, 
      method = criterion, names = names(predictor))
  
  if (criterion == "Cp") {
    optimal_criterion <- optimal_subset[[criterion]] %>% min()
    optimal_subset_idx <- optimal_subset[[criterion]] %>% which.min()
    
  } else {
    optimal_criterion <- optimal_subset[[criterion]] %>% max()
    optimal_subset_idx <- optimal_subset[[criterion]] %>% which.max()
  }
  
  return(list(Criterion = optimal_criterion , 
       Variable_Selection = optimal_subset$which[optimal_subset_idx,]))
  
}
```
This function serves as a wrapper around the leaps function. It performs 
best-subset variable selection and then prints out the optimal model diagnostic 
and the predictor variables included in the regression model.

## fit_glmnet
```{r }
fit_glmnet = function(df, alpha, outcome, lambda) {
  
  outcome_formula <- as.formula(paste(outcome, "~."))
  predictor <- model.matrix(outcome_formula, data = df)[,-1]
  predicted <- df %>% pull(outcome)
  
  model_fit <- 
    glmnet(predictor, predicted, lambda = lambda, alpha = alpha)
  
  model_cv <-
    cv.glmnet(predictor, predicted, lambda = lambda, alpha = alpha)
  
  lambda_opt = model_cv[["lambda.min"]]
  
  model_fit <-
    glmnet(predictor, predicted, lambda = lambda_opt, alpha = alpha)
  
  return(model_fit)
  
}
```
This function serves as a wrapper for the glmnet fitting processing and 
covers both the fitting and cross-validation process. It returns the glmnet 
object produced after fitting with optimal lambda value.

## rmse_glmnet
```{r }
rmse_glmnet = function(model, test, outcome) {
  outcome_formula <- as.formula(paste(outcome, "~."))
  predictor <- model.matrix(outcome_formula, data = test)[,-1]
  predictions <- predict.glmnet(model, model[["lambda"]], newx = predictor,
                               type = "response")
  
  predictions <- as.vector(predictions)
  observed <- test %>% 
    pull(outcome)
  
  return(caret::RMSE(predictions, observed))
  
}
```
This is a wrapper function for the glmnet prediction and evaluation process. 
Given a fitted glmnet model, a test dataset, and an outcome variable of 
interest, this function predicts the avlues in the test dataset and then 
calculates and returns the rmse. 


## check_model
```{r}
check_model = function(data, name) {
  
  resid_fit = 
    data |>
    ggplot(aes(y = resid, x = pred)) +
    geom_point() +
    geom_smooth(method = "lm") + 
    labs(title = paste("Residual vs. Fitted:", name),
         y = "Residual", 
         x = "Fitted Value")
  
  print(resid_fit)
  qqnorm(pull(data, resid), main = paste("QQ Plot: ", name))
  qqline(pull(data, resid), col = "red")
  
}
```
This is a plotting function to combine several different plot types that assess 
the assumptions of linear models.


# *Descriptive Statistics*

## Visualizations of Distributions
```{r distributions, fig.width = 20, fig.height = 20, warning = FALSE, message = FALSE, echo = FALSE}
merged_violence_df |>
  select(homicide_rate:alcohol_consumption_rate) |>
  ggpairs()
```

All variables are skewed right, except for HDI which is 
bimodal and slightly left-skewed. Hence, we decided to apply ln transformations 
and Box-Cox transformations to ensure normality of the data. 



## Data Transformations

### Natural Log Transform
```{r ln-transform, fig.width = 20, fig.height = 20, warning = FALSE}
ln_transform = function(value) {
  return(log(abs(value)))
}

ln_df = 
  merged_violence_df |>
  mutate(across(c(homicide_rate:alcohol_consumption_rate), 
                ln_transform))

ln_df |>
  select(homicide_rate:alcohol_consumption_rate) |>
  ggpairs()

```

### Box-Cox Transform
```{r boxcox, fig.width = 20, fig.height = 20, warning = FALSE}
boxcox_transform = function(value) {
  if (all(is.na(value))) {
    return(value) 
  }

  min_value = min(value, na.rm = TRUE) 
  if (min_value <= 0) {
    value = value + abs(min_value) + 0.00001  
  }

  if (length(unique(value)) == 1) {
    return(value) 
  }

  boxcox_result = MASS::boxcox(value ~ 1, plotit = FALSE)
  lambda = boxcox_result$x[which.max(boxcox_result$y)]  

  if(lambda != 0) {
    transformed_value = (value^lambda - 1) / lambda
  } else {
    transformed_value = log(value)
  }
  return(transformed_value)
}

boxcox_df = merged_violence_df |> 
  mutate(across(c(homicide_rate:alcohol_consumption_rate), 
                ~ boxcox_transform(.)))

boxcox_df |>
  select(homicide_rate:alcohol_consumption_rate) |>
  ggpairs()

```

Both natural log and boxcox transformations substantially normalized the distributions. 
Since boxcox resulted in more improved results as shown in the plots above, 
we decided to use the boxcox transformed data for regression. 

## Multicolliniearity Diagnostics

We used `cor()` to find the correlation between the eight predictors of 
homicide rate and violence. 

```{r }
cor_matrix = 
  cor(boxcox_df[, c("gdp", "inflation_rate", "unemployment_rate", "hdi",
                             "economic_crime_rate", "personnel_rate", "trafficked_victims", 
                             "alcohol_consumption_rate")], use = "pairwise.complete.obs")

ggcorrplot(cor_matrix, 
           method = "circle",  
           type = "lower",  
           lab = TRUE,        
           lab_size = 3,      
           colors = c("blue", "white", "red"), # Color scale (blue = negative, red = positive)
           title = "Correlation Heatmap"
)
```

There is moderate correlation between `hdi` and `alcohol_consumption_rate` (corr = 0.56), `hdi` and `economic_crime_rate` (corr = 0.57), and `gdp` and `trafficked_victims` (corr - 0.51). 

# *Data Pre-Processing*

## Define Lambda Range
```{r}
lambda = 10^(seq(-2, 2.75, 0.1))
```

## Approach to Pre-Processing Data

Prior to performing any model fitting, we split our initial dataset into 
separate homicide and violence datasets, each of which includes the outcome 
variable and all numeric predictor variables. Specific predictor and outcome 
dataframes, and an outcome matrix, are created for later use in linear models, 
as well as lasso.

### Pre-Processing for Homicide
```{r}
homicide_df =
  boxcox_df |> 
  ungroup() |> 
  select(-violence_rate, -year, -country, -region, -iso3_code) |> 
  drop_na()

homicide_matrix <- model.matrix(homicide_rate ~., data = homicide_df)[,-1]

homicide_predictors <- homicide_df %>% 
  select(-homicide_rate)

homicide_outcome <- homicide_df %>% 
  pull(homicide_rate)
```

### Pre-Processing for Violence
```{r }
violence_df =
  boxcox_df |> 
  ungroup() |> 
  select(-year, -homicide_rate, -country, -region, -iso3_code) |>
  drop_na()

violence_matrix <- model.matrix(violence_rate ~., data = violence_df)[,-1]
  
violence_predictors <- violence_df %>% 
  select(-violence_rate)

violence_outcome <- violence_df %>% 
  pull(violence_rate)

```

# *Baseline MLR with Additive Effects*

First, we decided to fit a baseline MLR with additive effects from all possible 
predictor variables to use as a point of reference for our modeling process. 
This step provides insight into which  
covariates contribute to the outcomes of interest. 

## Predict Homicide Rates
```{r }
baseline_MLR_hom <- lm(homicide_rate ~ gdp + inflation_rate + unemployment_rate +
                   hdi + economic_crime_rate + personnel_rate + trafficked_victims + 
                    alcohol_consumption_rate , data = homicide_df)

baseline_MLR_hom_fitted <- baseline_MLR_hom[["fitted.values"]]

baseline_MLR_hom %>% 
  broom::tidy() %>% 
  knitr::kable()
```
Results from baseline MLR show that that HDI, economic crime rate and personnel rate are significant predictors of homicide.

## Collinearity of Baseline Homicide Model 
```{r }
vif_baseline_hom_MLR <- car::vif(baseline_MLR_hom)
vif_baseline_hom_MLR %>% 
  tibble(
    variable = names(vif_baseline_hom_MLR),
    VIF = vif_baseline_hom_MLR
  ) %>% 
  select(variable, VIF) %>% 
  knitr::kable()

```

Results from VIF show that there is no serious multicollinearity issue (VIF < 5).

## Predict Violence Rates
```{r }
baseline_MLR_viol <- lm(violence_rate ~ gdp + inflation_rate + unemployment_rate +
                   hdi + economic_crime_rate + personnel_rate + trafficked_victims + 
                    alcohol_consumption_rate , data = violence_df)

baseline_MLR_viol_fitted <- baseline_MLR_viol[["fitted.values"]]

baseline_MLR_viol %>% 
  broom::tidy() %>% 
  knitr::kable()
```
Results from MLR show that `gdp`, `inflation_rate`, `unemployment_rate`, `hdi`, `economic_crime_rate`, and `alcohol_consumption_rate` are significant predictors of violence rate $\alpha$ = 0.01. 

## Collinearity of Baseline Violence Model
```{r }
vif_baseline_viol_MLR <- car::vif(baseline_MLR_viol)
vif_baseline_viol_MLR %>% 
  tibble(
    variable = names(vif_baseline_viol_MLR),
    VIF = vif_baseline_viol_MLR
  ) %>% 
  select(variable, VIF) %>% 
  knitr::kable()
```
Results from VIF show that there is no serious multicollinearity issue (VIF < 5).

# *Lasso*
We used lasso to as a feature selection tool to find the most important 
variables in predicting homicide rate and violence rate. 

## Predictors for Violence Rate
```{r }
lasso_violence_fit = 
  glmnet(violence_matrix, violence_outcome, lambda = lambda)

lasso_violence_cv = 
  cv.glmnet(violence_matrix, violence_outcome, lambda = lambda)

lambda_violence_opt = 
  lasso_violence_cv[["lambda.min"]]

lasso_violence_fit = 
  glmnet(violence_matrix, violence_outcome, lambda = lambda_violence_opt)

lasso_violence_fit |> 
  broom::tidy() |> 
  knitr::kable()

lasso_predict_violence <- 
  predict.glmnet(lasso_violence_fit, lambda_violence_opt, 
                 newx = violence_matrix, type = "response")
  
lasso_predict_violence <- as.vector(lasso_predict_violence)

lasso_violence_resid <- tibble(
  residuals = violence_outcome - lasso_predict_violence
)

```

The optimal lambda for violence rate is `r round(lambda_violence_opt, 2)`. 


## Predictors for Homicide Rate
```{r }
lasso_homicide_fit = 
  glmnet(homicide_matrix, homicide_outcome, lambda = lambda)

lasso_homicide_cv = 
  cv.glmnet(homicide_matrix, homicide_outcome, lambda = lambda)

lambda_homicide_opt = 
  lasso_homicide_cv[["lambda.min"]]

lasso_homicide_fit = 
  glmnet(homicide_matrix, homicide_outcome, lambda = lambda_homicide_opt)

lasso_homicide_fit |> 
  broom::tidy() |> 
  knitr::kable()

lasso_predict_homicide <- 
  predict.glmnet(lasso_homicide_fit, lambda_homicide_opt, 
                 newx = homicide_matrix, type = "response")
  
lasso_predict_homicide <- as.vector(lasso_predict_homicide)

lasso_homicide_resid <- tibble(
  residuals = homicide_outcome - lasso_predict_homicide
)

```

The optimal lambda for homicide rate is `r round(lambda_homicide_opt, 2)`.  

# *Criterion-Based Procedure*

## Best-Subset Regression for Violence 

### Predict Violence Rate with R-Squared
```{r }
violence_best_subset_rsq <- 
  best_subset(predictor = violence_predictors, outcome = violence_outcome, 
              criterion = "adjr2")

violence_best_subset_rsq
```

### Predict Violence Rate with Cp 
```{r }
violence_best_subset_Cp <- 
  best_subset(predictor = violence_predictors, outcome = violence_outcome, 
              criterion = "Cp")
violence_best_subset_Cp
```

### Collinearity of Best-Subset Violence Model
```{r }
subset_violence_lm <- lm(violence_rate ~ gdp + inflation_rate + unemployment_rate + hdi + economic_crime_rate + personnel_rate + alcohol_consumption_rate, data = violence_df)
summary(subset_violence_lm)
subset_violence_fitted <- subset_violence_lm[["fitted.values"]]


vif_subset_violence_model <- car::vif(subset_violence_lm) 

vif_subset_violence_model %>% 
  tibble(
    variable = names(vif_subset_violence_model),
    VIF = vif_subset_violence_model
  ) %>% 
  select(variable, VIF) %>% 
  knitr::kable()
```
All variables have VIF value below 5, suggesting that there is no 
multi-colinearity concerns.

### Interpretation

The results from both criterion-based procedures suggest that 
significant predictors for `violence_rate` are `gdp`, `inflation_rate`, 
`unemployment_rate`, `economic_crime_rate`, `personnel_rate`, `hdi` 
and `alcohol_consumption rate`, with a total of 7 predictors. 
This yields the most appropriate Cp value (6.94), which is approximately 
close to the number of predictors, and highest adjusted R-squared (0.46).

## Best Subset Regression for Homicide 

### Predict Homicide Rate with R-Squared 
```{r }
homicide_best_subset_rsq <- 
  best_subset(predictor = homicide_predictors, outcome = homicide_outcome, 
              criterion = "adjr2")

homicide_best_subset_rsq
```

### Predict Homicide Rate with Cp
```{r }
homicide_best_subset_Cp <- 
  best_subset(predictor = homicide_predictors, outcome = homicide_outcome, 
              criterion = "Cp")

homicide_best_subset_Cp
```

### Collinearity of Best-Subset Homicide Model
```{r }
subset_homicide_lm <- lm(homicide_rate ~ unemployment_rate + hdi + 
                           economic_crime_rate + personnel_rate, 
                         data = homicide_df)
summary(subset_homicide_lm)
subset_homicide_fitted <- subset_homicide_lm[["fitted.values"]]


vif_subset_homicide_model <- car::vif(subset_homicide_lm)
vif_subset_homicide_model %>% 
  tibble(
    variable = names(vif_subset_homicide_model),
    VIF = vif_subset_homicide_model
  ) %>% 
  select(variable, VIF) %>% 
  knitr::kable()
```
All variables have VIF value below 5, suggesting that there is no 
multi-colinearity issue.


## Interpretation
For `homicide_rate`, the results are not as straightforward. 
The best model based on R-squared (0.57) is the model with `unemployment_rate`, `hdi`, `economic_crime_rate` and `personnel_rate` as predictors. Howeover, based on the Cp criterion (2.51), it's the model with `hdi`, `economic_crime_rate` and `personnel_rate`.

Subsequently, we will choose four predictors since it has a relatively high adjusted R-Squared (0.57) and we were interested in a model that explains more variation in violence rates. 

# *Model Comparison*

## Create Training and Testing Datasets
```{r}
cv_df_violence <- 
  modelr::crossv_mc(violence_df, 100)

cv_df_violence <- cv_df_violence %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

cv_df_homicide <-
  modelr::crossv_mc(homicide_df, 100)

cv_df_homicide <- cv_df_homicide %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

```

## Fit Models

### Violence
```{r}
cv_df_violence <-
  cv_df_violence %>% 
  mutate(
    MLR_all_var = map(train, \(df) lm(violence_rate ~ ., data = df)),
    MLR_best_subset = map(train, \(df) lm(violence_rate ~ gdp + inflation_rate + 
                          unemployment_rate + hdi + economic_crime_rate + personnel_rate +
                           alcohol_consumption_rate, data = df)),
    lasso = map(train, \(df) fit_glmnet(df, outcome = "violence_rate", 
                                        alpha = 1,lambda = lambda))
  ) %>% 
  mutate(
    rmse_all_var = map2_dbl(
      MLR_all_var, test, \(mod, test) rmse(model = mod, data = test)),
    rmse_best_subset = map2_dbl(
      MLR_best_subset, test, \(mod, test) rmse(model = mod, data = test)),
    rmse_lasso = map2_dbl(
      lasso, test, \(mod, test) rmse_glmnet(mod, test, "violence_rate"))
  ) %>% 
  select(starts_with("rmse_"))

```

### Homicide
```{r}
cv_df_homicide <-
  cv_df_homicide %>% 
  mutate(
    MLR_all_var = map(train, \(df) lm(homicide_rate ~ ., data = df)),
    MLR_best_subset = map(train, \(df) lm(homicide_rate ~ unemployment_rate + 
                                            personnel_rate + hdi + economic_crime_rate, 
                                          data = df)),
    lasso = map(train, \(df) fit_glmnet(df, outcome = "homicide_rate", 
                                        alpha = 1,lambda = lambda))
  ) %>% 
  mutate(
    rmse_all_var = map2_dbl(
      MLR_all_var, test, \(mod, test) rmse(model = mod, data = test)),
    rmse_best_subset = map2_dbl(
      MLR_best_subset, test, \(mod, test) rmse(model = mod, data = test)),
    rmse_lasso = map2_dbl(
      lasso, test, \(mod, test) rmse_glmnet(mod, test, "homicide_rate"))
  ) %>% 
  select(starts_with("rmse_"))
```


## Comparing the RMSE

### Violence
```{r }
cv_df_violence %>%
  pivot_longer(
    everything(),
    names_to = "Model", 
    values_to = "RMSE",
    names_prefix = "rmse_"
  ) %>% 
  mutate(
    Model = fct_inorder(Model)
  ) %>% 
  ggplot(aes(x = Model, y = RMSE)) +
  geom_violin() +
  ggtitle("RMSE by Model for Predicting Violence Rate") +
  theme(plot.title = element_text(hjust = 0.5))
```

Examining the distribution of RMSEs for each of the 3 models for 
predicting violence rate, we see that they are relatively similar. It appears 
that the MLR including all predictor variables has greater spread in lower 
RMSE values compared to the other two models.

The similarity in performance among all 3 models likely relates to the few
number of predictor variables that exist in our studied dataset. In general,
each of the 3 models will not differ by many predictor variables, and therefore
perform similarly. Comparing the 3 models, the best subset regression model
may be considered optimal as it reduces model complexity, without the 
caveat of difficulty interpreting beta coefficients, which occurs 
with lasso models.


### Homicide
```{r }
cv_df_homicide %>%
  pivot_longer(
    everything(),
    names_to = "Model", 
    values_to = "RMSE",
    names_prefix = "rmse_"
  )%>% 
  mutate(
    Model = fct_inorder(Model)
  ) %>% 
  ggplot(aes(x = Model, y = RMSE)) +
  geom_violin() +
  ggtitle("RMSE by Model for Predicting Homicide Rate") +
  theme(plot.title = element_text(hjust = 0.5))
```

Examining the 3 models we used to predict homicide rate, we see that 
the distribution of RMSE across 100-fold cross-validation are nearly identical. 
This is likely in regard to the few predictor variables included in our dataset.
In particular, with such few predictor variables, lasso is likely not the most 
well-suited for this task, since there is not a serious need to reduce model 
complexity. Consequently, the number of variables included in each of the 3 
models is  similar, and the difference of 1 or 2 included predictors 
between each of the models does not produce a substantial change in model 
effectiveness. Comparing the 3 models, the best subset regression model 
may be considered the best as it reduces model complexity but maintains 
ease of interpretationof beta coefficients, having a slightly  greater 
spread in lower RMSE values compared to the other two models, which is not 
as noticeable. 


# *Model Diagnostics*

## All Variables included MLR

```{r, message = FALSE, warning = FALSE}
baseline_hom_resid_df = 
  homicide_df |>
  add_residuals(baseline_MLR_hom) |>
  add_predictions(baseline_MLR_hom) 

baseline_viol_resid_df = 
  violence_df |>
  add_residuals(baseline_MLR_viol) |>
  add_predictions(baseline_MLR_viol) 

check_model(baseline_hom_resid_df, "MLR All Predictors Homicide Rate") 
check_model(baseline_viol_resid_df, "MLR All Predictors Violence Rate")
```
Examining the QQ-plots for the MLRs with all predictor variables included, we 
notice that our assumption of normality of residuals may be violated, as 
there are many lower-bound and upper-bound outliers that do not fall along 
the reference line. For the plots of fitted values versus residuals, we 
see differing results based on whether homicide rate or violence rate is 
the outcome variable. With homicide as the outcome variable, we can see 
residuals oscillate above and below the reference line, indicating approximate 
linearity of the relationship between the predictors and the outcome variables. 
However, the greater variability in the residuals as fitted values increase, 
suggests that we are seeing heteroscedasticity. Regarding violence rate as the 
outcome variable, we see both issues with heteroscedasticity and linearity. 

## Best Subset MLR

```{r, message = FALSE, warning = FALSE}
subset_homicide_resid_df = 
  homicide_df |>
  add_residuals(subset_homicide_lm) |>
  add_predictions(subset_homicide_lm) 

subset_violence_resid_df = 
  violence_df |>
  add_residuals(subset_violence_lm) |>
  add_predictions(subset_violence_lm) 

check_model(subset_homicide_resid_df, "MLR Best-Subset Homicide Rate")
check_model(subset_violence_resid_df, "MLR Best-Subset Violence Rate")
```

Examining the QQ-plots for the MLRs using the best-subset of predictor variables, 
we once again notice that our assumption of normality of residuals may be in 
violation, as there appear to many lower-bound and upper-bound outliers that do 
not fall along the reference line. For the fitted versus residuals plot for 
homicide rate, we see violations of both linearity and homosecdascity. The 
equivalent plot for violence rate presents similar concerns regarding 
homoscedascity, but shows a greater spread of residuals along the horizontal 
reference line, and therefore better implies linearity of relationship between
predictor variables and outcome. 


## Lasso

```{r, message = FALSE, warning - FALSE}
lasso_homicide_resid_df =
  lasso_homicide_resid |> 
  pull(residuals) |>
  bind_cols(lasso_predict_homicide) |>
  rename(resid = "...1", pred = "...2")

lasso_violence_resid_df =
  lasso_violence_resid |> 
  pull(residuals) |>
  bind_cols(lasso_predict_violence) |>
  rename(resid = "...1", pred = "...2")

check_model(lasso_homicide_resid_df, "Lasso Homicide Rate")
check_model(lasso_violence_resid_df, "Lasso Violence Rate")
```

Consistent with results from the QQ-plots of the previous two MLR models, lasso 
models for both homicide rate and violence rate show potential violations of the 
assumption of normality, as many lower-bound and upper-bound outlier residuals
do not fall along the reference line. For the fitted versus residuals plot 
of homicide and violence rates, we see a very low possibility of heteroscedascity 
and non-linearity, since the horizontal line crosses the value of zero, but this 
acceptable since this deviation is very insignificant.

# *Conclusion*

Based on our comparisons of RMSE, we can conclude that our best-subset regression 
models are most accurate at predicting the outcomes: (1) Violence Rate and (2) 
Homicide Rate. 

The best-subset regression model for predicting violence 
rate yielded an adjusted r-squared of 
`r round(violence_best_subset_rsq[["Criterion"]], 2)` and included 
the following variables as predictors: GDP, inflation rate, economic crime rate, hdi, unemployment rate, personnel rate, and alcohol consumption rate. The 
best-subset regression model for predicting homicide rate returned an 
r-squared of `r round(homicide_best_subset_rsq[["Criterion"]], 2)`. The 
variables included in this model were unemployment rate, HDI, economic crime 
rate and personnel rate. 

**Below are some important interpretations of our findings, based on the best-subset regression model for predicting homicide rate:**

HDI (Coefficient = -18.50047), p-value < 2e-16:
A 1-unit increase in the HDI results in a decrease of 18.50047 units in the transformed homicide rate, holding all other variables constant. Since the p-value is very small, HDI has a strong negative effect on the transformed homicide rate.

Economic Crime Rate (Coefficient = 0.12511, p-value = 9.21e-08):
A 1-unit increase in transformed economic crime rate results in an increase of 0.12511 units in transformed homicide rate, holding all other variables constant. The p-value is very small, indicating statistical significance.

Personnel Rate (Coefficient = 0.09847, p-value = 0.000268):
A 1-unit increase in transformed personnel rate results in an increase of 0.09847 units in the transformed homicide rate, holding other predictors constant. The small p-value (0.000268) also makes this effect statistically significant. This is rather surprising, because we believed an increase in personnel rate should decrease homicide rate. However, because personnel rate is a variable that also includes judges, this result might make sense. 

**Here are some of our important findings based on the best-subset regression model for predicting violence rate:**

Economic Crime Rate (coefficient: 0.43184, p-value: < 2e-16)
A 1-unit increase in the transformed economic crime rate results in an increase of 0.43184 units in the transformed violence rate, holding all other variables constant. The p-value is extremely small, indicating a  significant relationship. This is consistent with the model predicting homicide rate.

GDP (coefficient: 0.33523, p-value: 3.22e-07)
A 1-unit increase in transformed GDP results in an increase of 0.33523 units in the transformed violence rate, holding all other variables constant. The p-value is very small, suggesting statistical significance. The association between economic growth and violence is surprising because we might previously associate higher crime with countries with lower GDP. However, these results may suggest that GDP growth can exacerbate inequality and drive violence. 

Alcohol Consumption Rate (coefficient: -0.19195, p-value: 0.000146)
A 1-unit increase in transformed alcohol consumption rate results in a decrease of 0.19195 units in the transformed violence rate, holding all other variables constant. The p-value is small (0.000146), indicating statistical significance. This result was also unexpected because we previously believed higher alcohol consumption would lead to an [increase in violence](https://pmc.ncbi.nlm.nih.gov/articles/PMC8729263/), since it's associated with aggression, mood and personality disorders). This might be due to 
limitations in our data quality. 
