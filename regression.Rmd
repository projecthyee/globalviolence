---
title: "**Regression**"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
library(tidyverse)
library(glmnet)
library(leaps)
library(mgcv)
library(ggcorrplot)
library(modelr)
library(GGally)

theme_set(theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5)))

set.seed(1)

merged_violence_df <- read_csv(
  file = "data/merged_violence.csv",
  col_select = c(2:15))
```

# *Background*
Our motivation for doing regression is to determine the significant predictors 
of the two measures of violence: (1) homicide victim rate and (2) violent offences rate. 

Global violence is a complex issue, depending on many factors, from political,
cultural, to economic. We aimed to choose predictors that reflect inequality which are
often related to social unrest and violence. Much numerical data exists for the economic indicators of a country
such as GDP, inflation and unemployment rate, and we also wanted to include HDI since that in itself
is a well-recognized global measurement. Intuitively, criminal activity is also related to violence,
so indicators for economic crime rate and criminal justice personnel were considered. Lastly, alcohol rate was included
because substance usage is often perceived to be correlated with violence. Subsequently, 
we aim to develop models that show which factors are the best predictors of homicide and violence rates. 

Given the plausible theoretical linkage between each of our included 
predictor variables and the outcome variables of interest, we seek to develop 
models that can determine which factors are most predictive of homicide and 
violence rate. 

We performed this analysis to uncover the extent to which 
different factors influence homicide rate and violence rate as outcomes.

# *Functions for Fitting Data*

## Best_subset
```{r }
best_subset = function(predictor, outcome, criterion) {
  
  optimal_subset <- 
    leaps(x = predictor, y = outcome, nbest = 3, 
      method = criterion, names = names(predictor))
  
  if (criterion == "Cp") {
    optimal_criterion <- optimal_subset[[criterion]] %>% min()
    optimal_subset_idx <- optimal_subset[[criterion]] %>% which.min()
    
  } else {
    optimal_criterion <- optimal_subset[[criterion]] %>% max()
    optimal_subset_idx <- optimal_subset[[criterion]] %>% which.max()
  }
  
  return(list(Criterion = optimal_criterion , 
       Variable_Selection = optimal_subset$which[optimal_subset_idx,]))
  
}
```
This function serves as a wrapper around the leaps function. It performs 
best-subset variable selection and then prints out the optimal model diagnostic 
and the predictor variables included in the regression model.

## fit_glmnet
```{r }
fit_glmnet = function(df, alpha, outcome, lambda) {
  
  outcome_formula <- as.formula(paste(outcome, "~."))
  predictor <- model.matrix(outcome_formula, data = df)[,-1]
  predicted <- df %>% pull(outcome)
  
  model_fit <- 
    glmnet(predictor, predicted, lambda = lambda, alpha = alpha)
  
  model_cv <-
    cv.glmnet(predictor, predicted, lambda = lambda, alpha = alpha)
  
  lambda_opt = model_cv[["lambda.min"]]
  
  model_fit <-
    glmnet(predictor, predicted, lambda = lambda_opt, alpha = alpha)
  
  return(model_fit)
  
}
```
This function serves as a wrapper for the glmnet fitting processing and 
covers both the fitting and cross-validation process. It returns the glmnet 
object produced after fitting with optimal lambda value.

## rmse_glmnet
```{r }
rmse_glmnet = function(model, test, outcome) {
  outcome_formula <- as.formula(paste(outcome, "~."))
  predictor <- model.matrix(outcome_formula, data = test)[,-1]
  predictions <- predict.glmnet(model, model[["lambda"]], newx = predictor,
                               type = "response")
  
  predictions <- as.vector(predictions)
  observed <- test %>% 
    pull(outcome)
  
  return(caret::RMSE(predictions, observed))
  
}
```
This is a wrapper function for the glmnet prediction and evaluation process. 
Given a fitted glmnet model, a test dataset, and an outcome variable of 
interest, this function predicts the avlues in the test dataset and then 
calculates and returns the rmse. 


## check_model
```{r}
check_model = function(data, name) {
  
  resid_fit = 
    data |>
    ggplot(aes(y = resid, x = pred)) +
    geom_point() +
    geom_smooth(method = "lm") + 
    labs(title = paste("Residual vs. Fitted:", name),
         y = "Residual", 
         x = "Fitted Value")
  
  print(resid_fit)
  qqnorm(pull(data, resid), main = paste("QQ Plot: ", name))
  qqline(pull(data, resid), col = "red")
  
}
```
This is a plotting function to combine several different plot types that assess 
the assumptions of linear models.


# *Descriptive Statistics*

## Visualizations of Distributions
```{r distributions, fig.width = 20, fig.height = 20, warning = FALSE, message = FALSE, echo = FALSE}
merged_violence_df |>
  select(homicide_rate:alcohol_consumption_rate) |>
  ggpairs()
```

All variables are skewed right, except for human development index which is 
bimodal and slightly left-skewed. Hence, we decided to apply ln transformations 
and Box-Cox transformations to ensure noramlity of the data. 



## Data Transformations

### Natural Log Transform
```{r ln-transform, fig.width = 20, fig.height = 20, warning = FALSE}
ln_transform = function(value) {
  return(log(abs(value)))
}

ln_df = 
  merged_violence_df |>
  mutate(across(c(homicide_rate:alcohol_consumption_rate), 
                ln_transform))

ln_df |>
  select(homicide_rate:alcohol_consumption_rate) |>
  ggpairs()

```

### Box-Cox Transform
```{r boxcox, fig.width = 20, fig.height = 20, warning = FALSE}
boxcox_transform = function(value) {
  if (all(is.na(value))) {
    return(value) 
  }

  min_value = min(value, na.rm = TRUE) 
  if (min_value <= 0) {
    value = value + abs(min_value) + 0.00001  
  }

  if (length(unique(value)) == 1) {
    return(value) 
  }

  boxcox_result = MASS::boxcox(value ~ 1, plotit = FALSE)
  lambda = boxcox_result$x[which.max(boxcox_result$y)]  

  if(lambda != 0) {
    transformed_value = (value^lambda - 1) / lambda
  } else {
    transformed_value = log(value)
  }
  return(transformed_value)
}

boxcox_df = merged_violence_df |> 
  mutate(across(c(homicide_rate:alcohol_consumption_rate), 
                ~ boxcox_transform(.)))

boxcox_df |>
  select(homicide_rate:alcohol_consumption_rate) |>
  ggpairs()

```


Since boxcox transformation substantially approved the shapes of the distributions, we decided to use the boxcox transformed data for regression.

## Multicolliniearity Diagnostics

We used `cor()` to find the correlation between the eight predictors of 
homicide rate and violence. 

```{r }
cor_matrix = 
  cor(boxcox_df[, c("gdp", "inflation_rate", "unemployment_rate", "hdi",
                             "economic_crime_rate", "personnel_rate", "trafficked_victims", 
                             "alcohol_consumption_rate")], use = "pairwise.complete.obs")

ggcorrplot(cor_matrix, 
           method = "circle",  
           type = "lower",  
           lab = TRUE,        
           lab_size = 3,      
           colors = c("blue", "white", "red"), # Color scale (blue = negative, red = positive)
           title = "Correlation Heatmap"
)
```

There is moderate correlation between `hdi` and `alcohol_consumption_rate` (corr = 0.56), `hdi` and `economic_crime_rate` (corr = 0.57), and `gdp` and `trafficked_victims` (corr - 0.51). 

# *Data Pre-Processing*

## Define Lambda Range
```{r}
lambda = 10^(seq(-2, 2.75, 0.1))
```

## Approach to Pre-Processing Data

Prior to performing any model fitting, we split our intial dataset into 
separate homicide and violence datasets, each of which includes the outcome 
variable and all numeric predictor variables. Specific predictor and outcome 
dataframes, and an outcome matrix, are created for later use in linear models, 
as well as lasso.

### Pre-Processing for Homicide
```{r}
homicide_df =
  boxcox_df |> 
  ungroup() |> 
  select(-violence_rate, -year, -country, -region, -iso3_code) |> 
  drop_na()

homicide_matrix <- model.matrix(homicide_rate ~., data = homicide_df)[,-1]

homicide_predictors <- homicide_df %>% 
  select(-homicide_rate)

homicide_outcome <- homicide_df %>% 
  pull(homicide_rate)
```

### Pre-Processing for Violence
```{r }
violence_df =
  boxcox_df |> 
  ungroup() |> 
  select(-year, -homicide_rate, -country, -region, -iso3_code) |>
  drop_na()

violence_matrix <- model.matrix(violence_rate ~., data = violence_df)[,-1]
  
violence_predictors <- violence_df %>% 
  select(-violence_rate)

violence_outcome <- violence_df %>% 
  pull(violence_rate)

```

# *Baseline MLR with Additive Effects*

First, we decided to fit a baseline MLR with additive effects from all possible 
predictor variables to use as a point of reference for our modeling process. 
This step provides basic understanding of the extent to which different 
covariates contribute to the outcomes of interest. 

## Predict Homicide Rates
```{r }
baseline_MLR_hom <- lm(homicide_rate ~ gdp + inflation_rate + unemployment_rate +
                   hdi + economic_crime_rate + personnel_rate + trafficked_victims + 
                    alcohol_consumption_rate , data = homicide_df)

baseline_MLR_hom_fitted <- baseline_MLR_hom[["fitted.values"]]

baseline_MLR_hom %>% 
  broom::tidy() %>% 
  knitr::kable()
```
Results from baseline MLR show that that HDI, economic crime rate and personnel rate are significant predictors of homicide.

## Collinearity of Baseline Homicide Model 
```{r }
vif_baseline_hom_MLR <- car::vif(baseline_MLR_hom)
vif_baseline_hom_MLR %>% 
  tibble(
    variable = names(vif_baseline_hom_MLR),
    VIF = vif_baseline_hom_MLR
  ) %>% 
  select(variable, VIF) %>% 
  knitr::kable()

```

Results from VIF show that there is no serious multicollinearity issue (VIF < 5).

## Predict Violence Rates
```{r }
baseline_MLR_viol <- lm(violence_rate ~ gdp + inflation_rate + unemployment_rate +
                   hdi + economic_crime_rate + personnel_rate + trafficked_victims + 
                    alcohol_consumption_rate , data = violence_df)

baseline_MLR_viol_fitted <- baseline_MLR_viol[["fitted.values"]]

baseline_MLR_viol %>% 
  broom::tidy() %>% 
  knitr::kable()
```
Results from MLR show that gdp, inflation rate, unemployment rate, hdi, economic crime rate, and alcohol consumption rates are significant predictors of violence rate at significance level alpha = 0.01. 

## Collinearity of Baseline Violence Model
```{r }
vif_baseline_viol_MLR <- car::vif(baseline_MLR_viol)
vif_baseline_viol_MLR %>% 
  tibble(
    variable = names(vif_baseline_viol_MLR),
    VIF = vif_baseline_viol_MLR
  ) %>% 
  select(variable, VIF) %>% 
  knitr::kable()
```
Results from VIF show that there is no serious multicollinearity issue (VIF < 5).

# *Lasso*
We used lasso to as a feature selection tool to find the most important 
variables in predicting homicide rate and violence rate. 

## Predictors for Violence Rate
```{r }
lasso_violence_fit = 
  glmnet(violence_matrix, violence_outcome, lambda = lambda)

lasso_violence_cv = 
  cv.glmnet(violence_matrix, violence_outcome, lambda = lambda)

lambda_violence_opt = 
  lasso_violence_cv[["lambda.min"]]

lasso_violence_fit = 
  glmnet(violence_matrix, violence_outcome, lambda = lambda_violence_opt)

lasso_violence_fit |> 
  broom::tidy() |> 
  knitr::kable()

lasso_predict_violence <- 
  predict.glmnet(lasso_violence_fit, lambda_violence_opt, 
                 newx = violence_matrix, type = "response")
  
lasso_predict_violence <- as.vector(lasso_predict_violence)

lasso_violence_resid <- tibble(
  residuals = violence_outcome - lasso_predict_violence
)

```

The optimal lambda for violence rate is `r round(lambda_violence_opt, 2)`. 


## Predictors for Homicide Rate
```{r }
lasso_homicide_fit = 
  glmnet(homicide_matrix, homicide_outcome, lambda = lambda)

lasso_homicide_cv = 
  cv.glmnet(homicide_matrix, homicide_outcome, lambda = lambda)

lambda_homicide_opt = 
  lasso_homicide_cv[["lambda.min"]]

lasso_homicide_fit = 
  glmnet(homicide_matrix, homicide_outcome, lambda = lambda_homicide_opt)

lasso_homicide_fit |> 
  broom::tidy() |> 
  knitr::kable()

lasso_predict_homicide <- 
  predict.glmnet(lasso_homicide_fit, lambda_homicide_opt, 
                 newx = homicide_matrix, type = "response")
  
lasso_predict_homicide <- as.vector(lasso_predict_homicide)

lasso_homicide_resid <- tibble(
  residuals = homicide_outcome - lasso_predict_homicide
)

```

The optimal lambda for homicide rate is `r round(lambda_homicide_opt, 2)`.  

# *Criterion-Based Procedure*

## Best-Subset Regression for Violence 

### Predict Violence Rate with R-Squared
```{r }
violence_best_subset_rsq <- 
  best_subset(predictor = violence_predictors, outcome = violence_outcome, 
              criterion = "adjr2")

violence_best_subset_rsq
```

### Predict Violence Rate with Cp 
```{r }
violence_best_subset_Cp <- 
  best_subset(predictor = violence_predictors, outcome = violence_outcome, 
              criterion = "Cp")
violence_best_subset_Cp
```

### Collinearity of Best-Subset Violence Model
```{r }
subset_violence_lm <- lm(violence_rate ~ gdp + inflation_rate + unemployment_rate + hdi + economic_crime_rate + personnel_rate + alcohol_consumption_rate, data = violence_df)
summary(subset_violence_lm)
subset_violence_fitted <- subset_violence_lm[["fitted.values"]]


vif_subset_violence_model <- car::vif(subset_violence_lm) 

vif_subset_violence_model %>% 
  tibble(
    variable = names(vif_subset_violence_model),
    VIF = vif_subset_violence_model
  ) %>% 
  select(variable, VIF) %>% 
  knitr::kable()
```
All variables have VIF value below 5, suggesting that there is no 
multi-colinearity concerns.

### Interpretation
The results from criterion-based procedures suggest that, based on the highest adjusted R-squared, the significant predictors for `violence_rate` are `gdp`, `inflation_rate`, `unemployment_rate`, `economic_crime_rate`, `hdi`, `personnel_rate`and `alcohol_consumption rate`, with a total of 7 predictors. 
The results from Cp, however, suggested a model that does not include personnel_rate. We ultimately decided to proceed with the model that does not include personnel rate, because in the real-world context, personnel rate should explain some variation in violence rate. 

## Best Subset Regression for Homicide 

### Predict Homicide Rate with R-Squared 
```{r }
homicide_best_subset_rsq <- 
  best_subset(predictor = homicide_predictors, outcome = homicide_outcome, 
              criterion = "adjr2")

homicide_best_subset_rsq
```

### Predict Homicide Rate with Cp
```{r }
homicide_best_subset_Cp <- 
  best_subset(predictor = homicide_predictors, outcome = homicide_outcome, 
              criterion = "Cp")

homicide_best_subset_Cp
```

### Collinearity of Best-Subset Homicide Model
```{r }
subset_homicide_lm <- lm(homicide_rate ~ unemployment_rate + hdi + 
                           economic_crime_rate + personnel_rate, 
                         data = homicide_df)
summary(subset_homicide_lm)
subset_homicide_fitted <- subset_homicide_lm[["fitted.values"]]


vif_subset_homicide_model <- car::vif(subset_homicide_lm)
vif_subset_homicide_model %>% 
  tibble(
    variable = names(vif_subset_homicide_model),
    VIF = vif_subset_homicide_model
  ) %>% 
  select(variable, VIF) %>% 
  knitr::kable()
```
All variables have VIF value below 5, suggesting that there is no 
multi-colinearity issue


## Interpretation
For `homicide_rate`, the results are not as straightforward. 
The best model based on R-squared (0.57) is the model with `unemployment_rate`, `hdi`, `economic_crime_rate` and `personnel_rate` as predictors. Howeover, based on the Cp criterion (2.51), it's the model with `hdi`, `economic_crime_rate` and `personnel_rate`.

Subsequently, we will choose four predictors since it has a relatively high adjusted R-Squared (0.57) and we are interested in a model that can explain more variation in violence rates. 

# *Model Comparison*

## Create Training and Testing Datasets
```{r}
cv_df_violence <- 
  modelr::crossv_mc(violence_df, 100)

cv_df_violence <- cv_df_violence %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

cv_df_homicide <-
  modelr::crossv_mc(homicide_df, 100)

cv_df_homicide <- cv_df_homicide %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

```

## Fit Models

### Violence
```{r}
cv_df_violence <-
  cv_df_violence %>% 
  mutate(
    MLR_all_var = map(train, \(df) lm(violence_rate ~ ., data = df)),
    MLR_best_subset = map(train, \(df) lm(violence_rate ~ gdp + inflation_rate + 
                          unemployment_rate + hdi + economic_crime_rate + personnel_rate +
                           alcohol_consumption_rate, data = df)),
    lasso = map(train, \(df) fit_glmnet(df, outcome = "violence_rate", 
                                        alpha = 1,lambda = lambda))
  ) %>% 
  mutate(
    rmse_all_var = map2_dbl(
      MLR_all_var, test, \(mod, test) rmse(model = mod, data = test)),
    rmse_best_subset = map2_dbl(
      MLR_best_subset, test, \(mod, test) rmse(model = mod, data = test)),
    rmse_lasso = map2_dbl(
      lasso, test, \(mod, test) rmse_glmnet(mod, test, "violence_rate"))
  ) %>% 
  select(starts_with("rmse_"))

```

### Homicide
```{r}
cv_df_homicide <-
  cv_df_homicide %>% 
  mutate(
    MLR_all_var = map(train, \(df) lm(homicide_rate ~ ., data = df)),
    MLR_best_subset = map(train, \(df) lm(homicide_rate ~ unemployment_rate + 
                                            personnel_rate + hdi + economic_crime_rate, 
                                          data = df)),
    lasso = map(train, \(df) fit_glmnet(df, outcome = "homicide_rate", 
                                        alpha = 1,lambda = lambda))
  ) %>% 
  mutate(
    rmse_all_var = map2_dbl(
      MLR_all_var, test, \(mod, test) rmse(model = mod, data = test)),
    rmse_best_subset = map2_dbl(
      MLR_best_subset, test, \(mod, test) rmse(model = mod, data = test)),
    rmse_lasso = map2_dbl(
      lasso, test, \(mod, test) rmse_glmnet(mod, test, "homicide_rate"))
  ) %>% 
  select(starts_with("rmse_"))
```


## Comparing the RMSE

### Violence
```{r }
cv_df_violence %>%
  pivot_longer(
    everything(),
    names_to = "Model", 
    values_to = "RMSE",
    names_prefix = "rmse_"
  ) %>% 
  mutate(
    Model = fct_inorder(Model)
  ) %>% 
  ggplot(aes(x = Model, y = RMSE)) +
  geom_violin() +
  ggtitle("RMSE by Model for Predicting Violence Rate") +
  theme(plot.title = element_text(hjust = 0.5))
```

Examining the distribution of RMSEs for each of the 3 models for 
predicting violence rate, we see that they are relatively similar. It appears 
that the MLR including all predictor variables has greater spread in lower 
RMSE values compared to the other two models.

The similarity in performance among all 3 models likely relates to the few
number of predictor variables that exist in our studied dataset. In general,
each of the 3 models will not differ by many predictor variables, and therefore
perform similarly. Comparing the 3 models, the best subset regression model
may be considered optimal as it reduces model complexity, without the 
further caveat of difficulty interpreting beta coefficients, as is the case 
with lasso models.


### Homicide
```{r }
cv_df_homicide %>%
  pivot_longer(
    everything(),
    names_to = "Model", 
    values_to = "RMSE",
    names_prefix = "rmse_"
  )%>% 
  mutate(
    Model = fct_inorder(Model)
  ) %>% 
  ggplot(aes(x = Model, y = RMSE)) +
  geom_violin() +
  ggtitle("RMSE by Model for Predicting Homicide Rate") +
  theme(plot.title = element_text(hjust = 0.5))
```

Examining the 3 models we used to predict homicide rate, we see that 
the distribution of RMSE across 100-fold cross-validation are nearly identical. 
This is likely in regard to the few predictor variables included in our dataset.
In particular, with such few predictor variables, lasso is likely not the most 
well-suited for this task, as there is not a serious need to reduce model 
complexity. Consequently, the number of variables included in each of the 3 
models is quite similar, and the difference of 1 or 2 included predictors 
between each of the models does not produce a substantial change in model 
effectiveness. Comparing the 3 models, the best subset regression model 
may be considered the best as it reduces model complexity but maintains 
ease of interpretability of beta coefficients, having a slighlty greater 
spread in lower RMSE values compared to the other two models, which is not 
as noticeable. 


# Model Diagnostics

## All Variables included MLR

```{r, message = FALSE, warning = FALSE}
baseline_hom_resid_df = 
  homicide_df |>
  add_residuals(baseline_MLR_hom) |>
  add_predictions(baseline_MLR_hom) 

baseline_viol_resid_df = 
  violence_df |>
  add_residuals(baseline_MLR_viol) |>
  add_predictions(baseline_MLR_viol) 

check_model(baseline_hom_resid_df, "MLR All Predictors Homicide Rate") 
check_model(baseline_viol_resid_df, "MLR All Predictors Violence Rate")
```
Examining the QQ-plots for the MLRs with all predictor variables included, we 
notice that our assumption of normality of residuals may be violated, as 
there are many lower-bound and upper-bound outliers that do not fall along 
the reference line. For the plots of fitted values versus residuals, we 
see differing results based on whether homicide rate or violence rate is 
the outcome variable. With homicide as the outcome variable, we can see 
residuals oscillate above and below the reference line, indicating approximate 
linearity of the relationship between the predictors and the outcome variables. 
However, the greater variability in the residuals as fitted values increase, 
suggests that we are seeing heteroscedasticity. Regarding violence rate as the 
outcome variable, we see both issues with heteroscedasticity and linearity. 

## Best Subset MLR

```{r, message = FALSE, warning = FALSE}
subset_homicide_resid_df = 
  homicide_df |>
  add_residuals(subset_homicide_lm) |>
  add_predictions(subset_homicide_lm) 

subset_violence_resid_df = 
  violence_df |>
  add_residuals(subset_violence_lm) |>
  add_predictions(subset_violence_lm) 

check_model(subset_homicide_resid_df, "MLR Best-Subset Homicide Rate")
check_model(subset_violence_resid_df, "MLR Best-Subset Violence Rate")
```

Examining the QQ-plots for the MLRs using the best-subset of predictor variables, 
we once again notice that our assumption of normality of residuals may be in 
violation, as there appear to many lower-bound and upper-bound outliers that do 
not fall along the reference line. For the fitted versus residuals plot for 
homicide rate, we see violations of both linearity and homosecdascity. The 
equivalent plot for violence rate presents similar concerns regarding 
homoscedascity, but shows a greater spread of residuals along the horizontal 
reference line, and therefore better implies linearity of relationship between
predictor variables and outcome. 


## Lasso

```{r, message = FALSE, warning - FALSE}
lasso_homicide_resid_df =
  lasso_homicide_resid |> 
  pull(residuals) |>
  bind_cols(lasso_predict_homicide) |>
  rename(resid = "...1", pred = "...2")

lasso_violence_resid_df =
  lasso_violence_resid |> 
  pull(residuals) |>
  bind_cols(lasso_predict_violence) |>
  rename(resid = "...1", pred = "...2")

check_model(lasso_homicide_resid_df, "Lasso Homicide Rate")
check_model(lasso_violence_resid_df, "Lasso Violence Rate")
```

Consistent with results from the QQ-plots of the previous two MLR models, lasso 
models for both homicide rate and violence rate show potential violations of the 
assumption of normality, as many lower-bound and upper-bound outlier residuals
do not fall along the reference line. For the fitted versus residuals plot 
of homicide and violence rates, we see a very low possibility of heteroscedascity 
and non-linearity, since the horizontal line crosses the value of zero, but this 
acceptable since this deviation is very insignificant.

# Conclusion

Based on our comparisons of RMSE, we can conclude that our best-subset regression 
models are most accurate at predicting the outcomes: (1) Violence Rate and (2) 
Homicide Rate. 

The best-subset regression model for predicting violence 
rate yielded an adjusted r-squared of 
`r round(violence_best_subset_rsq[["Criterion"]], 2)` and included 
the following variables a predictors: GDP, inflation rate, economic crime rate, hdi, unemployment rate, personnel rate, and alcohol consumption rate. The 
best-subset regression model for predicting homicide rate returned an 
r-squared of `r round(homicide_best_subset_rsq[["Criterion"]], 2)`. The 
variables included in this model were unemployment rate, HDI, economic crime 
rate and personnel rate. 

Below are some important interpretations of our findings, based on the best-subset regression model for predicting homicide rate:
HDI (Coefficient = -18.50047), p-value < 2e-16:
A 1-unit increase in the Human Development Index (HDI) results in a decrease of 18.50047 units in the transformed homicide rate, holding all other variables constant. Since the p-value is very small (< 2e-16), HDI has a strong negative effect on the transformed homicide rate.

Economic Crime Rate (Coefficient = 0.12511, p-value = 9.21e-08):
A 1-unit increase in transformed economic crime rate results in an increase of 0.12511 units in transformed homicide rate, holding all other variables constant. The p-value is very small (9.21e-08), indicating statistical significance.

Personnel Rate (Coefficient = 0.09847, p-value = 0.000268):
A 1-unit increase in transformed personnel rate results in an increase of 0.09847 units in the transformed homicide rate, holding other predictors constant. The small p-value (0.000268) also makes this effect statistically significant. This is rather surprising, because an increase in personnel rate should decrease homicide rate in theory. However, this finding could suggest that higher personnel rate may lead to more focused policing in some areas, which increase homicide reporting, or this could also reflect lack of personnel training, corruption or misallocation of resources.

Here are some of our important findings based on the best-subset regression model for predicting violence rate:

Economic Crime Rate (coefficient: 0.43184, p-value: < 2e-16)
A 1-unit increase in the transformed economic crime rate results in an increase of 0.43184 units in the transformed violence rate, holding all other variables constant. The p-value is extremely small (< 2e-16), indicating a highly significant relationship. This is consistent with the model predicting homicide rate.

GDP (coefficient: 0.33523, p-value: 3.22e-07)
A 1-unit increase in transformed GDP results in an increase of 0.33523 units in the transformed violence rate, holding all other variables constant. The p-value (3.22e-07) is very small, suggesting statistical significance. The association between economic growth and violence suggests that GDP growth may exacerbate inequality or create economic disparity, which might be a driver of violence. 

Alcohol Consumption Rate (coefficient: -0.19195, p-value: 0.000146)
A 1-unit increase in transformed alcohol consumption rate results in a decrease of 0.19195 units in the transformed violence rate, holding all other variables constant. The p-value is small (0.000146), indicating statistical significance. This result is rather counterintuitive, but in some settings, alcohol-related interventions and policies can help reduce violence (e.g., controlling alcohol abuse or fostering a safer drinking culture).