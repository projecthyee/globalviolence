<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Regression</title>

<script src="site_libs/header-attrs-2.28/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-6.4.2/css/all.min.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Home</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="data_cleaning.html">Data Cleaning</a>
</li>
<li>
  <a href="eda.html">EDA</a>
</li>
<li>
  <a href="regression.html">Regression</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Shiny &amp; Dashboard
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="https://yousmni3269.shinyapps.io/shiny_prevalence/">Prevalence of Violence</a>
    </li>
    <li>
      <a href="https://yousmni3269.shinyapps.io/shiny_trend/">Trends of Violence</a>
    </li>
    <li>
      <a href="plotly_vs.html">Homicide vs. Violence</a>
    </li>
  </ul>
</li>
<li>
  <a href="p8105_fp_report.html">Project Report</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="aboutus.html">About Us</a>
</li>
<li>
  <a href="https://github.com/projecthyee/p8105_fp_violence">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore"><strong>Regression</strong></h1>

</div>


<div id="background" class="section level1">
<h1><em>Background</em></h1>
<p>Our motivation for doing regression is to determine the significant
predictors of the two measures of violence: (1) homicide rate and (2)
violence rate.</p>
<p>Violence is a complex and multifaceted issue, especially when
considered in a global context. To better understand its various
dimensions, we aimed to include a diverse set of predictors that reflect
different underlying factors driving violence. Specifically, economic
indicators like GDP, inflation rate, and the Human Development Index
(HDI) capture aspects of economic inequality, which can influence social
unrest and violence. Economic crime rate, personnel rate, and number of
trafficked victims serve as indicators of the level of criminal activity
in a country, as well as potential efficacy of law enforcement. Such
indicators could be predictive of violence in the country. Lastly,
alcohol rate is included to address the possibility that substance usage
could influence the occurrence of violence.</p>
<p>Given the plausible theoretical linkage between each of our included
predictor variables and the outcome variables of interest, we seek to
develop models that can determine which factors are most predictive of
homicide and violence rate.</p>
<p>We performed this analysis to uncover the extent to which different
factors influence homicide rate and violence rate as outcomes.</p>
</div>
<div id="functions-for-fitting-data" class="section level1">
<h1><em>Functions for Fitting Data</em></h1>
<div id="best_subset" class="section level2">
<h2>Best_subset</h2>
<pre class="r"><code>best_subset = function(predictor, outcome, criterion) {
  
  optimal_subset &lt;- 
    leaps(x = predictor, y = outcome, nbest = 3, 
      method = criterion, names = names(predictor))
  
  if (criterion == &quot;Cp&quot;) {
    optimal_criterion &lt;- optimal_subset[[criterion]] %&gt;% min()
    optimal_subset_idx &lt;- optimal_subset[[criterion]] %&gt;% which.min()
    
  } else {
    optimal_criterion &lt;- optimal_subset[[criterion]] %&gt;% max()
    optimal_subset_idx &lt;- optimal_subset[[criterion]] %&gt;% which.max()
  }
  
  return(list(Criterion = optimal_criterion , 
       Variable_Selection = optimal_subset$which[optimal_subset_idx,]))
  
}</code></pre>
<p>This function serves as a wrapper around the leaps function. It
performs best-subset variable selection and then prints out the optimal
model diagnostic and the predictor variables included in the regression
model.</p>
</div>
<div id="fit_glmnet" class="section level2">
<h2>fit_glmnet</h2>
<pre class="r"><code>fit_glmnet = function(df, alpha, outcome, lambda) {
  
  outcome_formula &lt;- as.formula(paste(outcome, &quot;~.&quot;))
  predictor &lt;- model.matrix(outcome_formula, data = df)[,-1]
  predicted &lt;- df %&gt;% pull(outcome)
  
  model_fit &lt;- 
    glmnet(predictor, predicted, lambda = lambda, alpha = alpha)
  
  model_cv &lt;-
    cv.glmnet(predictor, predicted, lambda = lambda, alpha = alpha)
  
  lambda_opt = model_cv[[&quot;lambda.min&quot;]]
  
  model_fit &lt;-
    glmnet(predictor, predicted, lambda = lambda_opt, alpha = alpha)
  
  return(model_fit)
  
}</code></pre>
<p>This function serves as a wrapper for the glmnet fitting processing
and covers both the fitting and cross-validation process. It returns the
glmnet object produced after fitting with optimal lambda value.</p>
</div>
<div id="rmse_glmnet" class="section level2">
<h2>rmse_glmnet</h2>
<pre class="r"><code>rmse_glmnet = function(model, test, outcome) {
  outcome_formula &lt;- as.formula(paste(outcome, &quot;~.&quot;))
  predictor &lt;- model.matrix(outcome_formula, data = test)[,-1]
  predictions &lt;- predict.glmnet(model, model[[&quot;lambda&quot;]], newx = predictor,
                               type = &quot;response&quot;)
  
  predictions &lt;- as.vector(predictions)
  observed &lt;- test %&gt;% 
    pull(outcome)
  
  return(caret::RMSE(predictions, observed))
  
}</code></pre>
<p>This is a wrapper function for the glmnet prediction and evaluation
process. Given a fitted glmnet model, a test dataset, and an outcome
variable of interest, this function predicts the avlues in the test
dataset and then calculates and returns the rmse.</p>
</div>
<div id="check_model" class="section level2">
<h2>check_model</h2>
<pre class="r"><code>check_model = function(data, name) {
  
  diagnostics = list()
  
  resid_fit =
    data |&gt;
    ggplot(aes(y = resid, x = pred)) +
    geom_point() +
    geom_smooth(method = &quot;lm&quot;) + 
    labs(title = name,
         y = &quot;Residual&quot;, 
         x = &quot;Fitted Value&quot;)

  qqplot = 
    data |&gt;
      filter(is.finite(resid)) |&gt;
      ggplot(aes(sample = resid)) +
      stat_qq() +
      stat_qq_line(col = &quot;red&quot;) +
      xlab(&quot;Theoretical Quantiles&quot;) +
      ylab(&quot;Residuals&quot;) +
      ggtitle(name)
  
  qqnorm(pull(data, resid), main = &quot;QQ Plot of Residuals&quot;)
  qqline(pull(data, resid), col = &quot;red&quot;)
      
  diagnostics = append(diagnostics, list(qqplot, resid_fit))
  patchwork::wrap_plots(diagnostics)
}</code></pre>
<p>This is a plotting function to combine several different plot types
that assess the assumptions of linear models</p>
</div>
</div>
<div id="descriptive-statistics" class="section level1">
<h1><em>Descriptive Statistics</em></h1>
<div id="visualizations-of-distributions" class="section level2">
<h2>Visualizations of Distributions</h2>
<p><img src="regression_files/figure-html/distributions-1.png" width="1920" /></p>
<p>All variables are skewed right, except for human development index
which is bimodal and slightly left-skewed. Hence, we decided to apply ln
transformations and Box-Cox transformations to ensure noramlity of the
data.</p>
</div>
<div id="transformations" class="section level2">
<h2>Transformations</h2>
<p>Transformation step involves writing a function for natural log
transformation and a function for Box-Cox transformation. The functions
are mapped into the nested list col which includes all continuous
variables in the <code>merged_violence_df</code> dataset.</p>
</div>
<div id="natural-log-transformation" class="section level2">
<h2>Natural Log Transformation</h2>
<pre class="r"><code>ln_transform = function(value) {
  return(log(abs(value)))
}

ln_df = 
  merged_violence_df |&gt;
  mutate(across(c(homicide_rate:alcohol_consumption_rate), 
                ln_transform))

ln_df |&gt;
  select(homicide_rate:alcohol_consumption_rate) |&gt;
  ggpairs()</code></pre>
<p><img src="regression_files/figure-html/ln-transform-1.png" width="1920" /></p>
</div>
<div id="box-cox-transformation" class="section level2">
<h2>Box-Cox Transformation</h2>
<pre class="r"><code>boxcox_transform = function(value) {
  if (all(is.na(value))) {
    return(value) 
  }

  min_value = min(value, na.rm = TRUE) 
  if (min_value &lt;= 0) {
    value = value + abs(min_value) + 0.00001  
  }

  if (length(unique(value)) == 1) {
    return(value) 
  }

  boxcox_result = MASS::boxcox(value ~ 1, plotit = FALSE)
  lambda = boxcox_result$x[which.max(boxcox_result$y)]  

  if(lambda != 0) {
    transformed_value = (value^lambda - 1) / lambda
  } else {
    transformed_value = log(value)
  }
  return(transformed_value)
}

boxcox_df = merged_violence_df |&gt; 
  mutate(across(c(homicide_rate:alcohol_consumption_rate), 
                ~ boxcox_transform(.)))

boxcox_df |&gt;
  select(homicide_rate:alcohol_consumption_rate) |&gt;
  ggpairs()</code></pre>
<p><img src="regression_files/figure-html/boxcox-1.png" width="1920" /></p>
<p>While the initial EDA suggested that logarithmic and Box-Cox
transformations might be useful for improving normality, the results
indicate that these transformations are not entirely necessary. The
primary motivation for using these transformations is often to address
skewness. However, after reviewing the data and the results of the
transformations, we found that the original data already provides a
reasonable representation of the underlying distribution.</p>
<p>While the natural logarithm helped with normality, the Box-Cox
transformation did not substantially improve the dataâ€™s distribution.
Box-Cox transformation requires shifting data to be strictly positive,
which could introduce unnecessary complexity.</p>
<p>Box-Cox and ln transformations were performed with intentions of
improving the approximate normality of each of the predictor
distributions. However, neither transformation substantially approved
the shapes of the distributions. Since no major improvements in
normality were observed with the transformations, we ultimately decided
to use the original data. Concerns regarding the normality of the
distributions are alleviated by the Central Limit Theorem. With a
sufficiently large amount of data points, we can assume that the
sampling distribution is approximately normal. Using the convention that
n &gt; 30 allows for the assumption of approximate normality, our 310
observation dataset suffices to be supported by CLT.</p>
</div>
<div id="multicolliniearity-diagnostics" class="section level2">
<h2>Multicolliniearity Diagnostics</h2>
<p>We used <code>cor()</code> to find the correlation between the eight
predictors of homicide rate and violence.</p>
<pre class="r"><code>cor_matrix = 
  cor(merged_violence_df[, c(&quot;gdp&quot;, &quot;inflation_rate&quot;, &quot;unemployment_rate&quot;, &quot;hdi&quot;,
                             &quot;economic_crime_rate&quot;, &quot;personnel_rate&quot;, &quot;trafficked_victims&quot;, 
                             &quot;alcohol_consumption_rate&quot;)], 
      use = &quot;pairwise.complete.obs&quot;)


ggcorrplot(cor_matrix, 
           method = &quot;circle&quot;,  
           type = &quot;lower&quot;,  
           lab = TRUE,        
           lab_size = 3,      
           colors = c(&quot;blue&quot;, &quot;white&quot;, &quot;red&quot;), # Color scale (blue = negative, red = positive)
           title = &quot;Correlation Heatmap&quot;
)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>There is moderate correlation between <code>hdi</code> and
<code>alcohol_consumption_rate</code>, corr = 0.56, and moderately high
correlation between <code>trafficked_victims</code> and
<code>gdp</code>, corr = 0.77.</p>
</div>
</div>
<div id="data-pre-processing" class="section level1">
<h1><em>Data Pre-processing</em></h1>
<p>In this step, we manipulate the merged_violence_df in preparation for
later statistical modelling.</p>
<div id="define-lambda-range" class="section level2">
<h2>Define Lambda Range</h2>
<pre class="r"><code>lambda = 10^(seq(-2, 2.75, 0.1))</code></pre>
</div>
<div id="approach-to-pre-processing-data" class="section level2">
<h2>Approach to Pre-Processing Data</h2>
<p>Prior to performing any model fitting, we split our intial dataset
into separate homicide and violence datasets, each of which includes the
outcome variable and all numeric predictor variables. Specific predictor
and outcome dataframes, and an outcome matrix, are created for later use
in linear models, as well as lasso.</p>
</div>
<div id="pre-processing-for-homicide" class="section level2">
<h2>Pre-Processing for Homicide</h2>
<pre class="r"><code>homicide_df =
  merged_violence_df |&gt; 
  ungroup() |&gt; 
  select(
    homicide_rate, everything(), -violence_rate, -year, -country, -region, 
    -iso3_code) |&gt; 
  drop_na()

homicide_matrix &lt;- model.matrix(homicide_rate ~., data = homicide_df)[,-1]

homicide_predictors &lt;- homicide_df %&gt;% 
  select(-homicide_rate)

homicide_outcome &lt;- homicide_df %&gt;% 
  pull(homicide_rate)</code></pre>
</div>
<div id="pre-processing-for-violence" class="section level2">
<h2>Pre-Processing for Violence</h2>
<pre class="r"><code>violence_df =
  merged_violence_df |&gt; 
  ungroup() |&gt; 
  select(
    violence_rate, everything(), -year, -homicide_rate, -country, -region, 
    -iso3_code) |&gt;
  drop_na()

violence_matrix &lt;- model.matrix(violence_rate ~., data = violence_df)[,-1]
  
violence_predictors &lt;- violence_df %&gt;% 
  select(-violence_rate)

violence_outcome &lt;- violence_df %&gt;% 
  pull(violence_rate)</code></pre>
</div>
</div>
<div id="baseline-mlr-with-additive-effects" class="section level1">
<h1><em>Baseline MLR with Additive Effects</em></h1>
<p>First, we decided to fit a baseline MLR with additive effects from
all possible predictor variables to use as a point of reference for our
modeling process. This step provides basic understanding of the extent
to which different covariates contribute to the outcomes of
interest.</p>
<div id="predict-homicide-rates" class="section level2">
<h2>Predict Homicide Rates</h2>
<pre class="r"><code>baseline_MLR_hom &lt;- lm(homicide_rate ~ gdp + inflation_rate + unemployment_rate +
                   hdi + economic_crime_rate + personnel_rate + trafficked_victims + 
                    alcohol_consumption_rate , data = merged_violence_df)

baseline_MLR_hom_fitted &lt;- baseline_MLR_hom[[&quot;fitted.values&quot;]]

baseline_MLR_hom %&gt;% 
  broom::tidy() %&gt;% 
  knitr::kable()</code></pre>
<table>
<colgroup>
<col width="36%" />
<col width="17%" />
<col width="14%" />
<col width="16%" />
<col width="14%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">estimate</th>
<th align="right">std.error</th>
<th align="right">statistic</th>
<th align="right">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">77.1223904</td>
<td align="right">6.8949976</td>
<td align="right">11.1852672</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="left">gdp</td>
<td align="right">0.0000000</td>
<td align="right">0.0000000</td>
<td align="right">1.4401126</td>
<td align="right">0.1508748</td>
</tr>
<tr class="odd">
<td align="left">inflation_rate</td>
<td align="right">-0.7310164</td>
<td align="right">0.1997345</td>
<td align="right">-3.6599403</td>
<td align="right">0.0002978</td>
</tr>
<tr class="even">
<td align="left">unemployment_rate</td>
<td align="right">-0.3801340</td>
<td align="right">0.1437800</td>
<td align="right">-2.6438594</td>
<td align="right">0.0086262</td>
</tr>
<tr class="odd">
<td align="left">hdi</td>
<td align="right">-79.7616365</td>
<td align="right">9.3579304</td>
<td align="right">-8.5234270</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="left">economic_crime_rate</td>
<td align="right">0.0047858</td>
<td align="right">0.0037406</td>
<td align="right">1.2793927</td>
<td align="right">0.2017441</td>
</tr>
<tr class="odd">
<td align="left">personnel_rate</td>
<td align="right">0.0090816</td>
<td align="right">0.0064515</td>
<td align="right">1.4076778</td>
<td align="right">0.1602584</td>
</tr>
<tr class="even">
<td align="left">trafficked_victims</td>
<td align="right">-0.0004297</td>
<td align="right">0.0005839</td>
<td align="right">-0.7358718</td>
<td align="right">0.4623817</td>
</tr>
<tr class="odd">
<td align="left">alcohol_consumption_rate</td>
<td align="right">-0.2165605</td>
<td align="right">0.2052374</td>
<td align="right">-1.0551708</td>
<td align="right">0.2921934</td>
</tr>
</tbody>
</table>
<p>Results from baseline MLR show that that inflation rate, unemployment
rate and hdi are significant predictors of homicide rate at alpha =
0.05.</p>
</div>
<div id="check-baseline-homicide-model-for-collinearity"
class="section level2">
<h2>Check Baseline Homicide Model for Collinearity</h2>
<pre class="r"><code>vif_baseline_hom_MLR &lt;- car::vif(baseline_MLR_hom)
vif_baseline_hom_MLR %&gt;% 
  tibble(
    variable = names(vif_baseline_hom_MLR),
    VIF = vif_baseline_hom_MLR
  ) %&gt;% 
  select(variable, VIF) %&gt;% 
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">variable</th>
<th align="right">VIF</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">gdp</td>
<td align="right">2.396884</td>
</tr>
<tr class="even">
<td align="left">inflation_rate</td>
<td align="right">1.176607</td>
</tr>
<tr class="odd">
<td align="left">unemployment_rate</td>
<td align="right">1.164411</td>
</tr>
<tr class="even">
<td align="left">hdi</td>
<td align="right">2.455203</td>
</tr>
<tr class="odd">
<td align="left">economic_crime_rate</td>
<td align="right">1.700020</td>
</tr>
<tr class="even">
<td align="left">personnel_rate</td>
<td align="right">1.306548</td>
</tr>
<tr class="odd">
<td align="left">trafficked_victims</td>
<td align="right">2.315171</td>
</tr>
<tr class="even">
<td align="left">alcohol_consumption_rate</td>
<td align="right">1.896571</td>
</tr>
</tbody>
</table>
<p>Results from VIF show that there is no multicollinearity issue (VIF
&lt; 5).</p>
</div>
<div id="predict-violence-rates" class="section level2">
<h2>Predict Violence Rates</h2>
<pre class="r"><code>baseline_MLR_viol &lt;- lm(violence_rate ~ gdp + inflation_rate + unemployment_rate +
                   hdi + economic_crime_rate + personnel_rate + trafficked_victims + 
                    alcohol_consumption_rate , data = merged_violence_df)

baseline_MLR_viol_fitted &lt;- baseline_MLR_viol[[&quot;fitted.values&quot;]]

baseline_MLR_viol %&gt;% 
  broom::tidy() %&gt;% 
  knitr::kable()</code></pre>
<table>
<colgroup>
<col width="36%" />
<col width="17%" />
<col width="15%" />
<col width="15%" />
<col width="14%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">estimate</th>
<th align="right">std.error</th>
<th align="right">statistic</th>
<th align="right">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">44.2045930</td>
<td align="right">19.9655911</td>
<td align="right">2.2140388</td>
<td align="right">0.0275754</td>
</tr>
<tr class="even">
<td align="left">gdp</td>
<td align="right">0.0000000</td>
<td align="right">0.0000000</td>
<td align="right">2.7199674</td>
<td align="right">0.0069083</td>
</tr>
<tr class="odd">
<td align="left">inflation_rate</td>
<td align="right">-2.0163975</td>
<td align="right">0.5783639</td>
<td align="right">-3.4863818</td>
<td align="right">0.0005625</td>
</tr>
<tr class="even">
<td align="left">unemployment_rate</td>
<td align="right">-0.1171670</td>
<td align="right">0.4163384</td>
<td align="right">-0.2814226</td>
<td align="right">0.7785796</td>
</tr>
<tr class="odd">
<td align="left">hdi</td>
<td align="right">-12.9837171</td>
<td align="right">27.0974152</td>
<td align="right">-0.4791497</td>
<td align="right">0.6321801</td>
</tr>
<tr class="even">
<td align="left">economic_crime_rate</td>
<td align="right">0.1193201</td>
<td align="right">0.0108316</td>
<td align="right">11.0158771</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="left">personnel_rate</td>
<td align="right">-0.0628082</td>
<td align="right">0.0186813</td>
<td align="right">-3.3620841</td>
<td align="right">0.0008735</td>
</tr>
<tr class="even">
<td align="left">trafficked_victims</td>
<td align="right">0.0082359</td>
<td align="right">0.0016908</td>
<td align="right">4.8709129</td>
<td align="right">0.0000018</td>
</tr>
<tr class="odd">
<td align="left">alcohol_consumption_rate</td>
<td align="right">-1.6323824</td>
<td align="right">0.5942983</td>
<td align="right">-2.7467389</td>
<td align="right">0.0063817</td>
</tr>
</tbody>
</table>
<p>Results from MLR show that gdp, inflation rate, crime rate, personnel
rate, number of trafficked victims per 100,000 and alcohol consumption
rates are significant predictors of violence rate at significance level
alpha = 0.01.</p>
</div>
<div id="calculate-vif-for-baseline-violence-mlr"
class="section level2">
<h2>Calculate VIF for Baseline Violence MLR</h2>
<pre class="r"><code>vif_baseline_viol_MLR &lt;- car::vif(baseline_MLR_viol)
vif_baseline_viol_MLR %&gt;% 
  tibble(
    variable = names(vif_baseline_viol_MLR),
    VIF = vif_baseline_viol_MLR
  ) %&gt;% 
  select(variable, VIF) %&gt;% 
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">variable</th>
<th align="right">VIF</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">gdp</td>
<td align="right">2.396884</td>
</tr>
<tr class="even">
<td align="left">inflation_rate</td>
<td align="right">1.176607</td>
</tr>
<tr class="odd">
<td align="left">unemployment_rate</td>
<td align="right">1.164411</td>
</tr>
<tr class="even">
<td align="left">hdi</td>
<td align="right">2.455203</td>
</tr>
<tr class="odd">
<td align="left">economic_crime_rate</td>
<td align="right">1.700020</td>
</tr>
<tr class="even">
<td align="left">personnel_rate</td>
<td align="right">1.306548</td>
</tr>
<tr class="odd">
<td align="left">trafficked_victims</td>
<td align="right">2.315171</td>
</tr>
<tr class="even">
<td align="left">alcohol_consumption_rate</td>
<td align="right">1.896571</td>
</tr>
</tbody>
</table>
<p>Results from VIF show that there is no multicollinearity issue (VIF
&lt; 5).</p>
</div>
</div>
<div id="lasso" class="section level1">
<h1><em>Lasso</em></h1>
<p>We used lasso to as a feature selection tool to find the most
important variables in predicting homicide rate and violence rate.</p>
<div id="predictors-for-violence-rate" class="section level2">
<h2>Predictors for Violence Rate</h2>
<pre class="r"><code>lasso_violence_fit = 
  glmnet(violence_matrix, violence_outcome, lambda = lambda)

lasso_violence_cv = 
  cv.glmnet(violence_matrix, violence_outcome, lambda = lambda)

lambda_violence_opt = 
  lasso_violence_cv[[&quot;lambda.min&quot;]]

lasso_violence_fit = 
  glmnet(violence_matrix, violence_outcome, lambda = lambda_violence_opt)

lasso_violence_fit |&gt; 
  broom::tidy() |&gt; 
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">step</th>
<th align="right">estimate</th>
<th align="right">lambda</th>
<th align="right">dev.ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">1</td>
<td align="right">33.2327610</td>
<td align="right">0.3981072</td>
<td align="right">0.5192427</td>
</tr>
<tr class="even">
<td align="left">gdp</td>
<td align="right">1</td>
<td align="right">0.0000000</td>
<td align="right">0.3981072</td>
<td align="right">0.5192427</td>
</tr>
<tr class="odd">
<td align="left">inflation_rate</td>
<td align="right">1</td>
<td align="right">-1.7615771</td>
<td align="right">0.3981072</td>
<td align="right">0.5192427</td>
</tr>
<tr class="even">
<td align="left">unemployment_rate</td>
<td align="right">1</td>
<td align="right">-0.0409621</td>
<td align="right">0.3981072</td>
<td align="right">0.5192427</td>
</tr>
<tr class="odd">
<td align="left">hdi</td>
<td align="right">1</td>
<td align="right">-1.7103106</td>
<td align="right">0.3981072</td>
<td align="right">0.5192427</td>
</tr>
<tr class="even">
<td align="left">economic_crime_rate</td>
<td align="right">1</td>
<td align="right">0.1133362</td>
<td align="right">0.3981072</td>
<td align="right">0.5192427</td>
</tr>
<tr class="odd">
<td align="left">personnel_rate</td>
<td align="right">1</td>
<td align="right">-0.0548232</td>
<td align="right">0.3981072</td>
<td align="right">0.5192427</td>
</tr>
<tr class="even">
<td align="left">trafficked_victims</td>
<td align="right">1</td>
<td align="right">0.0080624</td>
<td align="right">0.3981072</td>
<td align="right">0.5192427</td>
</tr>
<tr class="odd">
<td align="left">alcohol_consumption_rate</td>
<td align="right">1</td>
<td align="right">-1.5437306</td>
<td align="right">0.3981072</td>
<td align="right">0.5192427</td>
</tr>
</tbody>
</table>
<pre class="r"><code>lasso_predict_violence &lt;- 
  predict.glmnet(lasso_violence_fit, lambda_violence_opt, 
                 newx = violence_matrix, type = &quot;response&quot;)
  
lasso_predict_violence &lt;- as.vector(lasso_predict_violence)

lasso_violence_resid &lt;- tibble(
  residuals = violence_outcome - lasso_predict_violence
)</code></pre>
<p>The optimal lambda for violence rate is 0.3981072. Based on lasso
estimates, the coefficient for <code>gdp</code> was shrunk to 0.</p>
</div>
<div id="predictors-for-homicide-rate" class="section level2">
<h2>Predictors for Homicide Rate</h2>
<pre class="r"><code>lasso_homicide_fit = 
  glmnet(homicide_matrix, homicide_outcome, lambda = lambda)

lasso_homicide_cv = 
  cv.glmnet(homicide_matrix, homicide_outcome, lambda = lambda)

lambda_homicide_opt = 
  lasso_homicide_cv[[&quot;lambda.min&quot;]]

lasso_homicide_fit = 
  glmnet(homicide_matrix, homicide_outcome, lambda = lambda_homicide_opt)

lasso_homicide_fit |&gt; 
  broom::tidy() |&gt; 
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">step</th>
<th align="right">estimate</th>
<th align="right">lambda</th>
<th align="right">dev.ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">1</td>
<td align="right">75.5833480</td>
<td align="right">0.0630957</td>
<td align="right">0.3524531</td>
</tr>
<tr class="even">
<td align="left">gdp</td>
<td align="right">1</td>
<td align="right">0.0000000</td>
<td align="right">0.0630957</td>
<td align="right">0.3524531</td>
</tr>
<tr class="odd">
<td align="left">inflation_rate</td>
<td align="right">1</td>
<td align="right">-0.6907875</td>
<td align="right">0.0630957</td>
<td align="right">0.3524531</td>
</tr>
<tr class="even">
<td align="left">unemployment_rate</td>
<td align="right">1</td>
<td align="right">-0.3553263</td>
<td align="right">0.0630957</td>
<td align="right">0.3524531</td>
</tr>
<tr class="odd">
<td align="left">hdi</td>
<td align="right">1</td>
<td align="right">-78.2143914</td>
<td align="right">0.0630957</td>
<td align="right">0.3524531</td>
</tr>
<tr class="even">
<td align="left">economic_crime_rate</td>
<td align="right">1</td>
<td align="right">0.0043479</td>
<td align="right">0.0630957</td>
<td align="right">0.3524531</td>
</tr>
<tr class="odd">
<td align="left">personnel_rate</td>
<td align="right">1</td>
<td align="right">0.0085533</td>
<td align="right">0.0630957</td>
<td align="right">0.3524531</td>
</tr>
<tr class="even">
<td align="left">trafficked_victims</td>
<td align="right">1</td>
<td align="right">-0.0002459</td>
<td align="right">0.0630957</td>
<td align="right">0.3524531</td>
</tr>
<tr class="odd">
<td align="left">alcohol_consumption_rate</td>
<td align="right">1</td>
<td align="right">-0.2056343</td>
<td align="right">0.0630957</td>
<td align="right">0.3524531</td>
</tr>
</tbody>
</table>
<pre class="r"><code>lasso_predict_homicide &lt;- 
  predict.glmnet(lasso_homicide_fit, lambda_homicide_opt, 
                 newx = homicide_matrix, type = &quot;response&quot;)
  
lasso_predict_homicide &lt;- as.vector(lasso_predict_homicide)

lasso_homicide_resid &lt;- tibble(
  residuals = homicide_outcome - lasso_predict_homicide
)</code></pre>
<p>The optimal lambda for homicide rate is 0.0630957. Similar to the
results from violence, the coefficient for <code>gdp</code> was shrunk
to 0.</p>
</div>
</div>
<div id="criterion-based-procedure" class="section level1">
<h1><em>Criterion-Based Procedure</em></h1>
<div id="best-subset-regression-for-violence" class="section level2">
<h2>Best-Subset Regression for Violence</h2>
<div id="predict-violence-rate-using-r-squared" class="section level3">
<h3>Predict Violence Rate using R-Squared</h3>
<pre class="r"><code>violence_best_subset_rsq &lt;- 
  best_subset(predictor = violence_predictors, outcome = violence_outcome, 
              criterion = &quot;adjr2&quot;)

violence_best_subset_rsq</code></pre>
<pre><code>## $Criterion
## [1] 0.5103599
## 
## $Variable_Selection
##                      gdp           inflation_rate        unemployment_rate 
##                     TRUE                     TRUE                    FALSE 
##                      hdi      economic_crime_rate           personnel_rate 
##                    FALSE                     TRUE                     TRUE 
##       trafficked_victims alcohol_consumption_rate 
##                     TRUE                     TRUE</code></pre>
</div>
<div id="predict-violence-rate-using-cp" class="section level3">
<h3>Predict Violence Rate using Cp</h3>
<pre class="r"><code>violence_best_subset_Cp &lt;- 
  best_subset(predictor = violence_predictors, outcome = violence_outcome, 
              criterion = &quot;Cp&quot;)
violence_best_subset_Cp</code></pre>
<pre><code>## $Criterion
## [1] 5.363491
## 
## $Variable_Selection
##                      gdp           inflation_rate        unemployment_rate 
##                     TRUE                     TRUE                    FALSE 
##                      hdi      economic_crime_rate           personnel_rate 
##                    FALSE                     TRUE                     TRUE 
##       trafficked_victims alcohol_consumption_rate 
##                     TRUE                     TRUE</code></pre>
</div>
<div id="collinearity-check-in-best-subset-violence-model"
class="section level3">
<h3>Collinearity check in Best-Subset Violence Model</h3>
<pre class="r"><code>subset_violence_lm &lt;- lm(violence_rate ~ gdp + inflation_rate + 
                           economic_crime_rate + personnel_rate + trafficked_victims +
                           alcohol_consumption_rate, data = violence_df)

subset_violence_fitted &lt;- subset_violence_lm[[&quot;fitted.values&quot;]]


vif_subset_violence_model &lt;- car::vif(subset_violence_lm) 

vif_subset_violence_model %&gt;% 
  tibble(
    variable = names(vif_subset_violence_model),
    VIF = vif_subset_violence_model
  ) %&gt;% 
  select(variable, VIF) %&gt;% 
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">variable</th>
<th align="right">VIF</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">gdp</td>
<td align="right">2.346184</td>
</tr>
<tr class="even">
<td align="left">inflation_rate</td>
<td align="right">1.070415</td>
</tr>
<tr class="odd">
<td align="left">economic_crime_rate</td>
<td align="right">1.373276</td>
</tr>
<tr class="even">
<td align="left">personnel_rate</td>
<td align="right">1.240083</td>
</tr>
<tr class="odd">
<td align="left">trafficked_victims</td>
<td align="right">2.282920</td>
</tr>
<tr class="even">
<td align="left">alcohol_consumption_rate</td>
<td align="right">1.168675</td>
</tr>
</tbody>
</table>
<p>All variables have VIF value below 5, suggesting that there is no
multi-colinearity concerns.</p>
</div>
<div id="interpretation" class="section level3">
<h3>Interpretation</h3>
<p>The results from criterion-based procedures suggest that significant
predictors for violence_rate are gdp, inflation_rate,
economic_crime_rate, personnel_rate, trafficked_victims and
alcohol_consumption rate, with a total of 6 predictors. This yields the
most appropriate Cp value (5.363), which is approximately close to the
number of predictors, and highest adjusted R-squared (0.510).</p>
</div>
</div>
<div id="best-subset-regression-for-homicide" class="section level2">
<h2>Best Subset Regression for Homicide</h2>
<div id="predict-homicide-rate-using-r-squared" class="section level3">
<h3>Predict Homicide Rate using R-Squared</h3>
<pre class="r"><code>homicide_best_subset_rsq &lt;- 
  best_subset(predictor = homicide_predictors, outcome = homicide_outcome, 
              criterion = &quot;adjr2&quot;)

homicide_best_subset_rsq</code></pre>
<pre><code>## $Criterion
## [1] 0.3367418
## 
## $Variable_Selection
##                      gdp           inflation_rate        unemployment_rate 
##                     TRUE                     TRUE                     TRUE 
##                      hdi      economic_crime_rate           personnel_rate 
##                     TRUE                     TRUE                     TRUE 
##       trafficked_victims alcohol_consumption_rate 
##                    FALSE                     TRUE</code></pre>
</div>
<div id="predict-homicide-rate-using-cp" class="section level3">
<h3>Predict Homicide Rate using Cp</h3>
<pre class="r"><code>homicide_best_subset_Cp &lt;- 
  best_subset(predictor = homicide_predictors, outcome = homicide_outcome, 
              criterion = &quot;Cp&quot;)

homicide_best_subset_Cp</code></pre>
<pre><code>## $Criterion
## [1] 6.511243
## 
## $Variable_Selection
##                      gdp           inflation_rate        unemployment_rate 
##                    FALSE                     TRUE                     TRUE 
##                      hdi      economic_crime_rate           personnel_rate 
##                     TRUE                     TRUE                    FALSE 
##       trafficked_victims alcohol_consumption_rate 
##                    FALSE                    FALSE</code></pre>
</div>
<div id="collinearity-check-in-best-subset-homicide-model"
class="section level3">
<h3>Collinearity Check in Best-Subset Homicide Model</h3>
<pre class="r"><code>subset_homicide_lm &lt;- lm(homicide_rate ~ inflation_rate + unemployment_rate +
                           hdi + economic_crime_rate, data = homicide_df)

subset_homicide_fitted &lt;- subset_homicide_lm[[&quot;fitted.values&quot;]]


vif_subset_homicide_model &lt;- car::vif(subset_homicide_lm)
vif_subset_homicide_model %&gt;% 
  tibble(
    variable = names(vif_subset_homicide_model),
    VIF = vif_subset_homicide_model
  ) %&gt;% 
  select(variable, VIF) %&gt;% 
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">variable</th>
<th align="right">VIF</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">inflation_rate</td>
<td align="right">1.169288</td>
</tr>
<tr class="even">
<td align="left">unemployment_rate</td>
<td align="right">1.034265</td>
</tr>
<tr class="odd">
<td align="left">hdi</td>
<td align="right">1.468355</td>
</tr>
<tr class="even">
<td align="left">economic_crime_rate</td>
<td align="right">1.316883</td>
</tr>
</tbody>
</table>
<p>All variables have VIF value below 5, suggesting that there is no
multi-colinearity concerns.</p>
</div>
<div id="interpretation-1" class="section level3">
<h3>Interpretation</h3>
<p>For homicide_rate, the results are not as straightforward. The best
model based on Cp and R-squared seems to be the model with 4 predictors
because it has the lowest Cp (6.36), and a relatively high adjusted
R-squared (0.331), indicating a good balance between fit and complexity.
As more predictors are added, R-squared adjusts slightly but Cp levels
off at around 6 predictors. The model with 4 predictors, which includes
inflation_rate, unemployment_rate, hdi, personnel_rate, seems to be a
better trade-off.</p>
</div>
</div>
</div>
<div id="model-comparison" class="section level1">
<h1><em>Model Comparison </em></h1>
<div id="create-training-and-testing-datasets" class="section level2">
<h2>Create Training and Testing Datasets</h2>
<pre class="r"><code>cv_df_violence &lt;- 
  modelr::crossv_mc(violence_df, 100)

cv_df_violence &lt;- cv_df_violence %&gt;% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

cv_df_homicide &lt;-
  modelr::crossv_mc(homicide_df, 100)

cv_df_homicide &lt;- cv_df_homicide %&gt;% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )</code></pre>
</div>
<div id="fit-models" class="section level2">
<h2>Fit Models</h2>
<div id="violence" class="section level3">
<h3>Violence</h3>
<pre class="r"><code>cv_df_violence &lt;-
  cv_df_violence %&gt;% 
  mutate(
    MLR_all_var = map(train, \(df) lm(violence_rate ~ ., data = df)),
    MLR_best_subset = map(train, \(df) lm(violence_rate ~ gdp + inflation_rate + 
                           economic_crime_rate + personnel_rate + trafficked_victims +
                           alcohol_consumption_rate, data = df)),
    lasso = map(train, \(df) fit_glmnet(df, outcome = &quot;violence_rate&quot;, 
                                        alpha = 1,lambda = lambda))
  ) %&gt;% 
  mutate(
    rmse_all_var = map2_dbl(
      MLR_all_var, test, \(mod, test) rmse(model = mod, data = test)),
    rmse_best_subset = map2_dbl(
      MLR_best_subset, test, \(mod, test) rmse(model = mod, data = test)),
    rmse_lasso = map2_dbl(
      lasso, test, \(mod, test) rmse_glmnet(mod, test, &quot;violence_rate&quot;))
  ) %&gt;% 
  select(starts_with(&quot;rmse_&quot;))</code></pre>
</div>
<div id="homicide" class="section level3">
<h3>Homicide</h3>
<pre class="r"><code>cv_df_homicide &lt;-
  cv_df_homicide %&gt;% 
  mutate(
    MLR_all_var = map(train, \(df) lm(homicide_rate ~ ., data = df)),
    MLR_best_subset = map(train, \(df) lm(homicide_rate ~ inflation_rate 
                          + unemployment_rate + hdi + economic_crime_rate, data = df)),
    lasso = map(train, \(df) fit_glmnet(df, outcome = &quot;homicide_rate&quot;, 
                                        alpha = 1,lambda = lambda))
  ) %&gt;% 
  mutate(
    rmse_all_var = map2_dbl(
      MLR_all_var, test, \(mod, test) rmse(model = mod, data = test)),
    rmse_best_subset = map2_dbl(
      MLR_best_subset, test, \(mod, test) rmse(model = mod, data = test)),
    rmse_lasso = map2_dbl(
      lasso, test, \(mod, test) rmse_glmnet(mod, test, &quot;homicide_rate&quot;))
  ) %&gt;% 
  select(starts_with(&quot;rmse_&quot;))</code></pre>
</div>
</div>
<div id="comparing-the-rmse" class="section level2">
<h2>Comparing the RMSE</h2>
<div id="violence-1" class="section level3">
<h3>Violence</h3>
<pre class="r"><code>cv_df_violence %&gt;%
  pivot_longer(
    everything(),
    names_to = &quot;Model&quot;, 
    values_to = &quot;RMSE&quot;,
    names_prefix = &quot;rmse_&quot;
  ) %&gt;% 
  mutate(
    Model = fct_inorder(Model)
  ) %&gt;% 
  ggplot(aes(x = Model, y = RMSE)) +
  geom_violin() +
  ggtitle(&quot;RMSE by Model for Predicting Violence Rate&quot;) +
  theme(plot.title = element_text(hjust = 0.5))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Examining the distribution of RMSEs for each of the 3 models for
predicting violence rate, we see that they are relatively similar. It
appears that the MLR including all predictor variables has greater
spread than the other two distributions, as it has a higher upper bound
in the violin plot.</p>
<p>The similarity in performance among all 3 models likely relates to
the few number of predictor variables that exist in our studied dataset.
In general, each of the 3 models will not differ by many predictor
variables, and therefore perform similarly. Comparing the 3 models, the
best subset regression model may be considered optimal as it reduces
model complexity, without the further caveat of difficulty interpreting
beta coefficients, as is the case with lasso models.</p>
</div>
<div id="homicide-1" class="section level3">
<h3>Homicide</h3>
<pre class="r"><code>cv_df_homicide %&gt;%
  pivot_longer(
    everything(),
    names_to = &quot;Model&quot;, 
    values_to = &quot;RMSE&quot;,
    names_prefix = &quot;rmse_&quot;
  )%&gt;% 
  mutate(
    Model = fct_inorder(Model)
  ) %&gt;% 
  ggplot(aes(x = Model, y = RMSE)) +
  geom_violin() +
  ggtitle(&quot;RMSE by Model for Predicting Homicide Rate&quot;) +
  theme(plot.title = element_text(hjust = 0.5))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>Examining the 3 models we used to predict homicide rate, we see that
the distribution of RMSE across 100-fold cross-validation are nearly
identical. This is likely in regard to the few predictor variables
included in our dataset. In particular, with such few predictor
variables, lasso is likely not the most well-suited for this task, as
there is not a serious need to reduce model complexity. Consequently,
the number of variables included in each of the 3 models is quite
similar, and the difference of 1 or 2 included predictors between each
of the models does not produce a substantial change in model
effectiveness. Comparing the 3 models, the best subset regression model
may be considered the best as it reduces model complexity but maintains
ease of interpretability of beta coefficients.</p>
</div>
</div>
</div>
<div id="model-diagnostics" class="section level1">
<h1><em>Model Diagnostics</em></h1>
<div id="all-variables-included-mlr" class="section level2">
<h2>All Variables included MLR</h2>
<pre class="r"><code>baseline_hom_resid_df = 
  merged_violence_df |&gt;
  add_residuals(baseline_MLR_hom) |&gt;
  add_predictions(baseline_MLR_hom) 

baseline_viol_resid_df = 
  merged_violence_df |&gt;
  add_residuals(baseline_MLR_viol) |&gt;
  add_predictions(baseline_MLR_viol) 

check_model(baseline_hom_resid_df, &quot;MLR All Predictors Homicide Rate&quot;) </code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-26-1.png" width="672" /><img src="regression_files/figure-html/unnamed-chunk-26-2.png" width="672" /></p>
<pre class="r"><code>check_model(baseline_viol_resid_df, &quot;MLR All Predictors Violence Rate&quot;)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-26-3.png" width="672" /><img src="regression_files/figure-html/unnamed-chunk-26-4.png" width="672" />
Examining the QQ-plots for the MLRs with all predictor variables
included, we notice that our assumption of normality of residuals may be
in violation, as there appear to many lower-bound and upper-bound
outliers that do not fall along the reference line. Regarding the plots
of fitted values versus residuals, we see differing results based on
whether homicide rate or violence rate is the outcome variable. With
homicide as the outcome variable, we can see residuals oscillate above
and below the reference line, indicating approximate linearity of the
relationship between the predictors and the outcome variables. However,
the greater variability in the residuals as fitted values increase,
suggests that we are seeing heteroscedasticity. Regarding violence rate
as the outcome variable, we see both issues with heteroscedasticity and
linearity.</p>
</div>
<div id="best-subset-mlr" class="section level2">
<h2>Best Subset MLR</h2>
<pre class="r"><code>subset_homicide_resid_df = 
  merged_violence_df |&gt;
  add_residuals(subset_homicide_lm) |&gt;
  add_predictions(subset_homicide_lm) 

subset_violence_resid_df = 
  merged_violence_df |&gt;
  add_residuals(subset_violence_lm) |&gt;
  add_predictions(subset_violence_lm) 

check_model(subset_homicide_resid_df, &quot;MLR Best-Subset Homicide Rate&quot;)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-27-1.png" width="672" /><img src="regression_files/figure-html/unnamed-chunk-27-2.png" width="672" /></p>
<pre class="r"><code>check_model(subset_violence_resid_df, &quot;MLR Best-Subset Violence Rate&quot;)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-27-3.png" width="672" /><img src="regression_files/figure-html/unnamed-chunk-27-4.png" width="672" /></p>
<p>Examining the QQ-plots for the MLRs using the best-subset of
predictor variables, we once again notice that our assumption of
normality of residuals may be in violation, as there appear to many
lower-bound and upper-bound outliers that do not fall along the
reference line. For the fitted versus residuals plot for homicide rate,
we see violations of both linearity and homosecdascity. The equivalent
plot for violence rate presents similar concerns regarding
homoscedascity, but shows a greater spread of residuals along the
horizontal reference line, and therefore better implies linearity of
relationship between predictor variables and outcome.</p>
</div>
<div id="lasso-1" class="section level2">
<h2>Lasso</h2>
<pre class="r"><code>lasso_homicide_resid_df =
  lasso_homicide_resid |&gt; 
  pull(residuals) |&gt;
  bind_cols(lasso_predict_homicide) |&gt;
  rename(resid = &quot;...1&quot;, pred = &quot;...2&quot;)

lasso_violence_resid_df =
  lasso_violence_resid |&gt; 
  pull(residuals) |&gt;
  bind_cols(lasso_predict_violence) |&gt;
  rename(resid = &quot;...1&quot;, pred = &quot;...2&quot;)

check_model(lasso_homicide_resid_df, &quot;Lasso Homicide Rate&quot;)</code></pre>
<p><img src="regression_files/figure-html/warning-FALSE-1.png" width="672" /><img src="regression_files/figure-html/warning-FALSE-2.png" width="672" /></p>
<pre class="r"><code>check_model(lasso_violence_resid_df, &quot;Lasso Violence Rate&quot;)</code></pre>
<p><img src="regression_files/figure-html/warning-FALSE-3.png" width="672" /><img src="regression_files/figure-html/warning-FALSE-4.png" width="672" /></p>
<p>Consistent with results from the QQ-plots of the previous two MLR
models, lasso models for both homicide rate and violence rate show
potential violations of the assumption of normality, as many lower-bound
and upper-bound outlier residuals do not fall along the reference line.
For the fitted versus residuals plot of homicide rate, we see possible
heteroscedascity, while the equivalent plot for homicide rate shows both
possible heteroscedascity and non-linearity.</p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1><em>Conclusion</em></h1>
<p>Based on our comparisons of RMSE, we can conclude that our
best-subset regression models are most accurate at predicting the
outcomes: (1) Violence Rate and (2) Homicide Rate.</p>
<p>The best-subset regression model for predicting violence rate yielded
an adjusted r-squared of and included the following variables a
predictors: GDP, inflation rate, crime rate, personnel rate, trafficked
victims, and alcohol consumption rate. The best-subset regression model
for predicting homicide rate returned an r-squared of . The variables
included in this model were inflation rate, unemployment rate, HDI, and
crime rate.</p>
<p>Despite moderately high R-squared values for each of these models, we
must acknowledge that the modelsâ€™ performance and accuracy may suffer
from certain drawbacks. In particular, model diagnostics revealed
concerns regarding the normality, linearity, and homoscedascity of the
dataset used to produce these models. Additionally, transformations
applied to cope with the concerns regarding normality failed to produce
more normalized distributions for any of the predictor variables. As a
result, the R-squared values for our models may be inaccurate and
different forms of modelling, such as a non-linear approach, may be
better suited for exploring what variables predict violence and homicide
rates.</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
